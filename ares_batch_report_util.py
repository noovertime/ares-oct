# ares_batch_report_util.py (ìˆ˜ì •ëœ ì „ì²´ ì†ŒìŠ¤)

import os
import json
import time
from typing import Dict, List, Any, Tuple
from scipy.stats import binom
from scipy.optimize import brentq

import config

# ===================================================================
# 0. ì „ì—­ ìƒìˆ˜
# ===================================================================

CI_ALPHA = 0.05  # 95% ì‹ ë¢°êµ¬ê°„ (alpha=0.05)


# ===================================================================
# 1. í†µê³„ ê³„ì‚° ìœ í‹¸ë¦¬í‹° (í•¨ìˆ˜ ë‚´ìš©ì€ ìƒëµ)
# ===================================================================
# (calculate_binomial_ci ë° analyze_ppi_file í•¨ìˆ˜ ì •ì˜ ë¸”ë¡ì€ ìƒë‹¨ ì†ŒìŠ¤ì™€ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ)
# ...

def calculate_binomial_ci(n: int, k: int, alpha: float = 0.05) -> Tuple[float, float]:
    """
    ppi.pyì˜ binomial_iid ë¡œì§ì„ ì‚¬ìš©í•˜ì—¬ 95% ì´í•­ ì‹ ë¢°êµ¬ê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    if n == 0:
        return 0.0, 1.0

    muhat = k / n

    def invert_lower_tail(mu):
        return binom.cdf(k, n, mu) - (1 - alpha / 2)

    def invert_upper_tail(mu):
        return binom.cdf(k, n, mu) - (alpha / 2)

    if k == n:
        u = 1.0
    else:
        try:
            u = brentq(invert_upper_tail, muhat, 1.0)
        except ValueError:
            u = 1.0

    if k == 0:
        l = 0.0
    else:
        try:
            l = brentq(invert_lower_tail, 0.0, muhat)
        except ValueError:
            l = 0.0

    return l, u


def analyze_ppi_file(filepath: str, ppi_correction_active: bool,
                     gold_fields: Dict[str, str]) -> Dict[str, Any]:
    """
    ë‹¨ì¼ PPI íŒŒì¼ì„ ì½ì–´ PPI ë³´ì • í‰ê·  ë° ì‹ ë¢°êµ¬ê°„ì„ ê³„ì‚°í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.
    """
    if not ppi_correction_active:
        raise RuntimeError("PPI ë³´ì •ì„ ìœ„í•œ ê³¨ë“  ë¼ë²¨ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    model_name_raw = os.path.basename(filepath)
    all_scores: Dict[str, List[int]] = {
        'contextrelevance': [],
        'answerfaithfulness': [],
        'answerrelevance': []
    }

    rectifier_terms: Dict[str, List[float]] = {'contextrelevance': [], 'answerfaithfulness': [], 'answerrelevance': []}

    model_name_parts = model_name_raw.split('_')
    model_name = "_".join(model_name_parts[:-1])
    if not model_name:
        model_name = model_name_raw.replace(".jsonl", "")

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line.strip())

                    for key in all_scores.keys():
                        if key in data:
                            all_scores[key].append(data[key])

                    for machine_key, gold_key in gold_fields.items():
                        machine_pred = data.get(machine_key)
                        gold_label = data.get(gold_key)

                        if gold_label in [0, 1] and machine_pred in [0, 1]:
                            rectifier_terms[machine_key].append(machine_pred - gold_label)

                except json.JSONDecodeError:
                    continue
    except FileNotFoundError:
        return None

    total_n = len(all_scores['contextrelevance'])
    if total_n == 0:
        return None

    summary = {
        'model_name': model_name,
        'n': total_n,
        'ppi_active': ppi_correction_active
    }

    overall_corrected_scores = []

    for axis in all_scores.keys():
        scores = all_scores[axis]
        success_k = sum(scores)

        machine_mean = success_k / total_n

        if rectifier_terms[axis]:
            rectifier = sum(rectifier_terms[axis]) / len(rectifier_terms[axis])
        else:
            rectifier = 0.0

        corrected_mean = max(0.0, min(1.0, machine_mean - rectifier))

        l, u = calculate_binomial_ci(total_n, success_k)
        margin = (u - l) / 2

        summary[axis] = {
            'machine_mean': round(machine_mean, 2),
            'corrected_mean': round(corrected_mean, 2),
            'ci': round(margin, 3)
        }
        overall_corrected_scores.append(corrected_mean)

    summary['overall'] = round(sum(overall_corrected_scores) / 3, 2)
    summary['applied_rectifier'] = round(rectifier, 3)

    return summary


# ===================================================================
# 2. ë³´ê³ ì„œ ìƒì„± ë° ì¶œë ¥ (ì»¬ëŸ¼ ì„¤ëª… ì¶”ê°€)
# ===================================================================

def generate_summary_report(model_summaries: List[Dict[str, Any]]):
    """
    ë¶„ì„ëœ ëª¨ë¸ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… Markdown í˜•ì‹ì˜ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """

    if not model_summaries:
        return "[WARN] ë¶„ì„í•  ëª¨ë¸ ë°ì´í„°ê°€ ì—†ì–´ ë³´ê³ ì„œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    current_time = time.strftime("%Y-%m-%d %H:%M:%S")

    model_summaries.sort(key=lambda x: x['overall'], reverse=True)

    report_title = "PPI ë³´ì • ê²°ê³¼ (ê³¨ë“  ë¼ë²¨ ê¸°ë°˜)"
    rectifier_note = "ê³¨ë“  ë¼ë²¨(Gold Label) ê¸°ë°˜ì˜ ì‹¤ì œ í¸í–¥(Rectifier Mean)ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤."

    report_content = f"""
# ğŸ§­ ARES ìë™ í‰ê°€ ê²°ê³¼ ë³´ê³ ì„œ ({report_title})

í”„ë¡œì íŠ¸ëª…: ARES ì‹¬ì‚¬ê´€ ë¡œì»¬ ë°°ì¹˜ í‰ê°€
í‰ê°€ í”„ë ˆì„ì›Œí¬: Stanford ARES (PPI ë³´ì • ë¡œì§ í†µí•©)
í‰ê°€ ì¼ì: {current_time}
í‰ê°€ ëŒ€ìƒ ëª¨ë¸: {', '.join([s['model_name'] for s in model_summaries])}

### 1ï¸âƒ£ í‰ê°€ ê°œìš” (PPI ë³´ì • ê¸°ë°˜)

| í‰ê°€ ì¶• | ë¯¸ì„¸ë¶€ ì„¤ëª… |
| :--- | :--- |
| **Context Relevance (CR)** | ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ê°€ (ë¬¸ë§¥ ì í•©ì„±) |
| **Answer Faithfulness (AF)** | ìƒì„±ëœ ë‹µë³€ì´ ê²€ìƒ‰ ë¬¸ì„œ ë‚´ìš©ì— ì¶©ì‹¤í•œê°€ (ì‘ë‹µ ì¶©ì‹¤ë„) |
| **Answer Relevance (AR)** | ë‹µë³€ì´ ì§ˆë¬¸ì— ì§ì ‘ì ì´ê³  êµ¬ì²´ì ì¸ê°€ (ì‘ë‹µ ì ì ˆì„±) |

### 2ï¸âƒ£ PPI ë³´ì • ì ìˆ˜ ë° ì‹ ë¢°êµ¬ê°„ ìš”ì•½ (95% CI)

| ëª¨ë¸ëª… | **CR (ë³´ì •)** | AF (ë³´ì •) | AR (ë³´ì •) | **ì¢…í•© ì ìˆ˜** | ê¸°ê³„ ì˜ˆì¸¡ í‰ê·  | ì ìš©ëœ í¸í–¥ | CI ë§ˆì§„ (Â±) | ì´ ìƒ˜í”Œ ìˆ˜ |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
"""

    for summary in model_summaries:
        cr_str = f"{summary['contextrelevance']['corrected_mean']:.2f}"
        af_str = f"{summary['answerfaithfulness']['corrected_mean']:.2f}"
        ar_str = f"{summary['answerrelevance']['corrected_mean']:.2f}"
        ci_margin = f"Â±{summary['contextrelevance']['ci']:.3f}"

        report_content += (
            f"| **{summary['model_name']}** "
            f"| {cr_str} "
            f"| {af_str} "
            f"| {ar_str} "
            f"| **{summary['overall']:.2f}** "
            f"| {summary['contextrelevance']['machine_mean']:.2f} "
            f"| {summary['applied_rectifier']:.3f} "
            f"| {ci_margin} "
            f"| {summary['n']} |\n"
        )

    report_content += f"""
---
#### ğŸ’¡ ìš”ì•½ í‘œ ì»¬ëŸ¼ ì˜ë¯¸ ì„¤ëª…

| ì»¬ëŸ¼ëª… | ì„¤ëª… |
| :--- | :--- |
| **CR, AF, AR (ë³´ì •)** | PPI ë³´ì • ë¡œì§ì„ ê±°ì³ **í¸í–¥ì´ ì œê±°ëœ** ê° ì¶•ì˜ ìµœì¢… ì„±ëŠ¥ ì¶”ì •ì¹˜ì…ë‹ˆë‹¤. (0.00 ~ 1.00) |
| **ì¢…í•© ì ìˆ˜** | CR, AF, AR ì„¸ ê°€ì§€ ë³´ì • ì ìˆ˜ì˜ í‰ê· ì…ë‹ˆë‹¤. |
| **ê¸°ê³„ ì˜ˆì¸¡ í‰ê· ** | ARES ì‹¬ì‚¬ê´€ì´ ì‹¤ì œë¡œ ì˜ˆì¸¡í•œ ì ìˆ˜($\hat{{Y}}$)ì˜ ë‹¨ìˆœ í‰ê· ì…ë‹ˆë‹¤. (ë³´ì • ì „ ì ìˆ˜) |
| **ì ìš©ëœ í¸í–¥** | ê¸°ê³„ ì˜ˆì¸¡ í‰ê· ($\hat{{Y}}$)ì—ì„œ ë³´ì • ì ìˆ˜ë¥¼ ì–»ê¸° ìœ„í•´ ì°¨ê°ëœ **í¸í–¥ ê°’**($\hat{{Y}} - Y$)ì…ë‹ˆë‹¤. |
| **CI ë§ˆì§„ (Â±)** | **95% ì‹ ë¢°êµ¬ê°„(CI)**ì˜ ë§ˆì§„ì…ë‹ˆë‹¤. ì‹¤ì œ ì„±ëŠ¥ì€ (**ê¸°ê³„ ì˜ˆì¸¡ í‰ê· ** $\pm$ **CI ë§ˆì§„**) ë²”ìœ„ ë‚´ì— ì¡´ì¬í•  í™•ë¥ ì´ 95%ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. |
| **ì´ ìƒ˜í”Œ ìˆ˜** | í‰ê°€ì— ì‚¬ìš©ëœ Q-C-A íŠ¸ë¦¬í”Œì˜ ì „ì²´ ê°œìˆ˜ì…ë‹ˆë‹¤. |
---
### 3ï¸âƒ£ ë³´ì • ë¡œì§ ë° í•´ì„

* **PPI ë³´ì • ì ìš©:** PPI(Prediction-Powered Inference) ë°©ì‹ì— ë”°ë¼ **ë³´ì • í‰ê·  (Corrected Mean)**ì„ ì‚°ì¶œí–ˆìŠµë‹ˆë‹¤.
* **PPI ë¶„ì‚° ê³„ì‚° ë°©ì‹:** ARESëŠ” PPI í‰ê·  ì¶”ì •ì˜ **ì ê·¼ì  ë¶„ì‚° ê³µì‹**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë³´ê³ ì„œì˜ CI ë§ˆì§„ì€ í˜„ì¬ **ì´í•­ ì‹ ë¢°êµ¬ê°„ ê·¼ì‚¬ì¹˜**ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë©°, PPIì˜ ì—„ê²©í•œ ë¶„ì‚° ê³„ì‚°(ê¸°ê³„ ì˜ˆì¸¡ ë¶„ì‚° ë° ë³´ì •í•­ ë¶„ì‚°)ì€ ê³¨ë“  ë¼ë²¨ ì œê³µ í›„ ì¶”ê°€ í†µí•©ë  ì˜ˆì •ì…ë‹ˆë‹¤.
* **ë³´ì • ë°©ë²•:** {rectifier_note}
"""

    return report_content


def run_summary_generation_pipeline(ppi_correction_active: bool, gold_fields: Dict[str, str]):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ë¥¼ ìœ„í•´ ë¶„ì„/ì €ì¥ ë¡œì§ë§Œ ë¶„ë¦¬í•©ë‹ˆë‹¤."""

    # ... (í•¨ìˆ˜ ë‚´ìš© ìœ ì§€) ...
    if not ppi_correction_active:
        raise RuntimeError("PPI ë³´ì •ì„ ìœ„í•œ ê³¨ë“  ë¼ë²¨ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ë³´ê³ ì„œ ìƒì„±ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    INPUT_DIR_SUM = config.DATA_OUT_DIR
    OUTPUT_DIR_SUM = config.DATA_REPORT_DIR

    os.makedirs(OUTPUT_DIR_SUM, exist_ok=True)

    ppi_files = [
        os.path.join(INPUT_DIR_SUM, f)
        for f in os.listdir(INPUT_DIR_SUM)
        if f.endswith('.jsonl') and len(f.split('_')) >= 2
    ]

    if not ppi_files:
        print(f"[WARN] {INPUT_DIR_SUM}ì—ì„œ ë¶„ì„í•  PPI íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë³´ê³ ì„œ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    print(f"[INFO] ì´ {len(ppi_files)}ê°œ ëª¨ë¸ì˜ PPI íŒŒì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

    model_summaries = []

    for file_path in ppi_files:
        summary = analyze_ppi_file(file_path, True, gold_fields)
        if summary:
            model_summaries.append(summary)
            print(f"   [SUCCESS] ëª¨ë¸ '{summary['model_name']}' ë¶„ì„ ì™„ë£Œ.")

    if not model_summaries:
        print("[WARN] ë¶„ì„ ê°€ëŠ¥í•œ ëª¨ë¸ ë°ì´í„°ê°€ ì—†ì–´ ë³´ê³ ì„œ ìƒì„±ì„ ê±´ë„ˆstms.")
        return

    report_content = generate_summary_report(model_summaries)

    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_filename = f"report_{timestamp}.md"
    output_path = os.path.join(OUTPUT_DIR_SUM, output_filename)

    with open(output_path, 'w', encoding='utf-8') as outfile:
        outfile.write(report_content)

    print("\n\n=============== ARES í†µê³„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ ===============")
    print(f"**í†µê³„ ë³´ê³ ì„œ ì €ì¥ ê²½ë¡œ:** {output_path}")
    print("==========================================================")