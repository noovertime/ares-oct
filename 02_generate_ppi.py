# 02_generate_ppi.py (í´ë˜ìŠ¤ ê¸°ë°˜ ë° ë‹¤ì¤‘ ê³¨ë“ ì…‹ í™•ì¥ ë²„ì „)
# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import abc
import json
import logging
import math
import os
import time
from typing import Dict, Any, List, Tuple

# 2. ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
from numpy.typing import NDArray
import torch
from scipy.stats import norm
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# ONNX Runtime ë° Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬
from onnxruntime import InferenceSession, SessionOptions
from transformers import AutoTokenizer

# Transformers ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™”
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)
logging.getLogger("transformers.modeling_utils").setLevel(logging.ERROR)

# 3. ë¡œì»¬/ì• í”Œë¦¬ì¼€ì´ì…˜ ê³ ìœ  ë¼ì´ë¸ŒëŸ¬ë¦¬
import config
import report_util
from config import (
    KEY_CR,
    KEY_AF,
    KEY_AR,
    JUDGE_TYPES,
    JUDGE_PREDICTION_FIELDS,
    GOLD_LABEL_FIELDS,
    TOKEN_TYPE_ID_OFF
)
from json_util import _load_json_lines  # ì™¸ë¶€ ëª¨ë“ˆ ê°€ì •

# ===================================================================
# 0. ì „ì—­ ìƒìˆ˜ ë° í™˜ê²½ ì„¤ì •
# ===================================================================

# Transformers ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™”
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)
logging.getLogger("transformers.modeling_utils").setLevel(logging.ERROR)

# ê²½ë¡œ ì„¤ì •
MODEL_DIR_BASE = config.MODEL_DIR
MODEL_NAME = config.MODEL_NAME

# CPU í™˜ê²½ ì„¤ì •
DEVICE = torch.device("cpu")
MAX_LENGTH = 128

# ARES ì‹¬ì‚¬ê´€ íƒ€ì… ë° í´ë” ë§¤í•‘
FOLDER_MAPPING = {
    config.KEY_CR: 'context_relevance',
    config.KEY_AF: 'answer_faithfulness',
    config.KEY_AR: 'answer_relevance'
}

CI_ALPHA: float = 0.05  # 95% ì‹ ë¢°êµ¬ê°„
CI_Z_SCORE: float = float(norm.ppf(1 - CI_ALPHA / 2))  # ì•½ 1.96


# í‰ê°€ ë°°ì¹˜ í¬ê¸° ì„¤ì • (í´ìˆ˜ë¡ ì†ë„ í–¥ìƒ, ë©”ëª¨ë¦¬ ì‚¬ìš© ì¦ê°€)
BATCH_SIZE = 32
#
ENGINE_TYPE_ONNX_RUNTIME = "ONNX Runtime"

# ===================================================================
# 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (í´ë˜ìŠ¤ì— í¬í•¨ì‹œí‚¤ì§€ ì•ŠëŠ” ë²”ìš© ê¸°ëŠ¥)
# ===================================================================

def _find_model_path(judge_type: str) -> str:
    """ê³ ì •ëœ ì‹¬ì‚¬ê´€ ì´ë¦„ í´ë” ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # judge_typeì€ KEY_CR, KEY_AF, KEY_AR ì¤‘ í•˜ë‚˜
    target_folder = FOLDER_MAPPING.get(judge_type)

    if not target_folder:
        raise ValueError(f"ì •ì˜ë˜ì§€ ì•Šì€ ì‹¬ì‚¬ê´€ íƒ€ì…: {judge_type}")

    model_path = os.path.join(MODEL_DIR_BASE, target_folder)

    if not os.path.isdir(model_path):
        raise FileNotFoundError(f"ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    print(f"ëª¨ë¸ : {judge_type} -> {model_path}")

    return model_path


def cleanup_evaluation_data():
    """
    configì— ì •ì˜ëœ PPI ì¶œë ¥ ë””ë ‰í† ë¦¬ì™€ ìµœì¢… ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    dirs_to_clean = [ config.DATA_REPORT_DIR]
    print("\n>> í‰ê°€ ë° ë³´ê³ ì„œ ì¶œë ¥ íŒŒì¼ ì •ë¦¬ ì‹œì‘...")
    for target_dir in dirs_to_clean:
        if os.path.isdir(target_dir):
            files_deleted = 0

            for filename in os.listdir(target_dir):
                file_path = os.path.join(target_dir, filename)
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        files_deleted += 1
                except Exception as e:
                    print(f"   [ERROR] íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")

            print(f"   [SUCCESS] '{target_dir}' ë””ë ‰í† ë¦¬ì—ì„œ ì´ {files_deleted}ê°œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ.")
        else:
            print(f"   [INFO] ë””ë ‰í† ë¦¬ '{target_dir}'ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì •ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")


# ===================================================================
# 2. ARES í‰ê°€ í´ë˜ìŠ¤ (ì¶”ìƒí™” ë²„ì „)
# ===================================================================
class AresJudge(abc.ABC):
    """ëª¨ë¸ ë¡œë”© ë° í‰ê°€ë¥¼ ìœ„í•œ ì¶”ìƒ ìŠˆí¼ í´ë˜ìŠ¤."""

    def __init__(self, device: torch.device, max_length: int, model_type_name: str, engine_type:str) -> None:
        self.tokenizer: AutoTokenizer | None = None
        self.judges: Dict[str, Any] = {}  # PyTorch ëª¨ë¸ ë˜ëŠ” ONNX InferenceSessionì„ ë‹´ìŒ
        self.device = device
        self.max_length = max_length
        self.model_type_name = model_type_name
        self.engine_type: str = engine_type
        self._load_models()

    @abc.abstractmethod
    def _load_models(self) -> None:
        """ê° ì„œë¸Œ í´ë˜ìŠ¤ì—ì„œ ì—”ì§„ì— ë§ê²Œ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•„ìˆ˜ ë©”ì„œë“œ."""
        pass

    @abc.abstractmethod
    def _get_logits(self, judge_type: str, inputs: Dict[str, Any],
                    tokenized_data: Dict[str, Any]) -> np.ndarray | torch.Tensor:
        """ì—”ì§„ íƒ€ì…ì— ë”°ë¼ ì¶”ë¡ ì„ ì‹¤í–‰í•˜ê³  Logitsë¥¼ ë°˜í™˜í•˜ëŠ” í•„ìˆ˜ ë©”ì„œë“œ."""
        pass

    def evaluate_triple(self, query: str, context: str, answer: str) -> Dict[str, Dict[str, float | int]]:
        """
        í•˜ë‚˜ì˜ Q-C-A ìŒì— ëŒ€í•´ CR, AF, AR ì ìˆ˜(0/1)ì™€ Softmax í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ê³µí†µ ë¡œì§.
        """
        results: Dict[str, Dict[str, float | int]] = {}

        # ê°ê°ì´ ì–´ë–¤ ê²ƒì„ í‰ê°€í• ì§€ ë§µí•‘ ê´€ê³„
        judge_inputs_map = {
            KEY_CR: (query, context),
            KEY_AF: (context, answer),
            KEY_AR: (query, answer)
        }

        for judge_type in JUDGE_TYPES:
            text_a, text_b = judge_inputs_map[judge_type]

            # 1. í† í°í™” (NumPy í…ì„œ ë˜ëŠ” PyTorch í…ì„œ)
            return_tensors = "np" if self.engine_type == ENGINE_TYPE_ONNX_RUNTIME else "pt"

            tokenized_data = self.tokenizer(
                text_a, text_b,
                return_tensors=return_tensors,
                truncation=True,
                padding="max_length",
                max_length=self.max_length,
            )

            # 2. Logits ê³„ì‚° (ì„œë¸Œ í´ë˜ìŠ¤ í˜¸ì¶œ)
            logits_tensor = self._get_logits(judge_type, tokenized_data, tokenized_data)

            # 3. ê²°ê³¼ ì‚°ì¶œ (ê³µí†µ í›„ì²˜ë¦¬)
            probabilities = torch.softmax(logits_tensor, dim=1).squeeze().cpu().numpy()
            prediction = int(torch.argmax(logits_tensor, dim=1).item())

            prob_neg = round(float(probabilities[0]), 4)
            prob_pos = round(float(probabilities[1]), 4)

            results[JUDGE_PREDICTION_FIELDS[judge_type]] = {
                'machine_pred': prediction,
                'prob_neg': prob_neg,
                'prob_pos': prob_pos
            }

        return results


class AresPytorchJudge(AresJudge):
    """BERT ë° DistilBERTì™€ ê°™ì€ PyTorch ê¸°ë°˜ ëª¨ë¸ ë¡œë”© ë° í‰ê°€ ë‹´ë‹¹."""

    def __init__(self, device, max_length, model_type_name) -> None:
        super().__init__(device, max_length, model_type_name, "PyTorch")

    def _get_model_path_base(self, judge_type: str) -> str:
        """ëª¨ë¸ í´ë” ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        target_folder = FOLDER_MAPPING.get(judge_type)
        if not target_folder: raise ValueError(f"ì •ì˜ë˜ì§€ ì•Šì€ ì‹¬ì‚¬ê´€ íƒ€ì…: {judge_type}")
        model_path = os.path.join(MODEL_DIR_BASE, target_folder)
        if not os.path.isdir(model_path): raise FileNotFoundError(f"ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return model_path

    def _load_models(self) -> None:
        """PyTorch ëª¨ë¸ ë¡œë”© ë¡œì§."""
        print(f"\n>> ARES ì‹¬ì‚¬ê´€ ë¡œë”© ì‹œì‘ ({self.model_type_name} / {self.engine_type} ì—”ì§„)...")

        # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
        cr_path = self._get_model_path_base(KEY_CR)
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(cr_path, trust_remote_code=True)
            print(f"[INFO] í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ: {cr_path}")
        except Exception:
            print(f"[WARN] í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨. ì›ë³¸ ëª¨ë¸ ({MODEL_NAME}) ë¡œë“œ ì‹œë„.")
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
            print(f"[INFO] ê¸°ë³¸ ëª¨ë¸ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ.")

        # DistilBERT í˜¸í™˜: token_type_ids ì œê±°
        if TOKEN_TYPE_ID_OFF and self.tokenizer:
            self.tokenizer.model_input_names = [
                n for n in self.tokenizer.model_input_names if n != "token_type_ids"
            ]
            print("[INFO] 'token_type_ids' ë¹„í™œì„±í™” ì™„ë£Œ")

        # 2. PyTorch ëª¨ë¸ ë¡œë“œ
        for judge_type in JUDGE_TYPES:
            try:
                model_folder_path = self._get_model_path_base(judge_type)
                model = AutoModelForSequenceClassification.from_pretrained(
                    model_folder_path, num_labels=2, trust_remote_code=True
                ).to(self.device)
                model.eval()
                self.judges[judge_type] = model
                print(f"[SUCCESS] {judge_type} Judge ë¡œë“œ ì™„ë£Œ ({self.engine_type})")
            except Exception as e:
                print(f"[ERROR] {judge_type} Judge ë¡œë“œ ì‹¤íŒ¨: {e}")

        if len(self.judges) != len(JUDGE_TYPES):
            raise RuntimeError(f"í•„ìš”í•œ PyTorch ëª¨ë¸ {len(JUDGE_TYPES)}ê°œ ì¤‘ {len(self.judges)}ê°œë§Œ ë¡œë“œë¨.")

        print(f">> ARES ì‹¬ì‚¬ê´€ ë¡œë“œ ì™„ë£Œ. ì´ {len(self.judges)}ê°œ ì‹¬ì‚¬ê´€ í™œì„±í™”.")

    def _get_logits(self, judge_type: str, inputs: Dict[str, Any], tokenized_data: Dict[str, Any]) -> torch.Tensor:
        """PyTorch ì¶”ë¡  ë¡œì§: ëª¨ë¸ì„ ì‹¤í–‰í•˜ê³  Logitsë¥¼ ë°˜í™˜."""
        model = self.judges[judge_type]

        with torch.no_grad():
            # PyTorch í…ì„œë¥¼ GPU/CPU ì¥ì¹˜ë¡œ ì´ë™
            pt_inputs = {k: v.to(self.device) for k, v in tokenized_data.items()}
            outputs = model.forward(**pt_inputs)
            return outputs.logits

    # evaluate_triple ë©”ì„œë“œëŠ” AresJudge ìŠˆí¼ í´ë˜ìŠ¤ì—ì„œ ìƒì† ë° ì¬í™œìš©ë¨


class AresSbertOnnxJudge(AresJudge):
    """SBERT ONNX Runtime ê¸°ë°˜ ëª¨ë¸ ë¡œë”© ë° í‰ê°€ ë‹´ë‹¹."""
    def __init__(self, device: torch.device,  max_length: int, model_type_name) -> None:
        super().__init__(device, max_length, model_type_name, ENGINE_TYPE_ONNX_RUNTIME)

    def _get_model_path_base(self, judge_type: str) -> str:
        """ëª¨ë¸ í´ë” ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        target_folder = FOLDER_MAPPING.get(judge_type)
        if not target_folder: raise ValueError(f"ì •ì˜ë˜ì§€ ì•Šì€ ì‹¬ì‚¬ê´€ íƒ€ì…: {judge_type}")
        model_path = os.path.join(MODEL_DIR_BASE, target_folder)
        if not os.path.isdir(model_path): raise FileNotFoundError(f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return model_path

    def _load_models(self) -> None:
        """ONNX InferenceSession ë¡œë”© ë¡œì§."""
        print(f"\n>> ARES ì‹¬ì‚¬ê´€ ë¡œë”© ì‹œì‘ ({self.model_type_name} / {self.engine_type} ì—”ì§„)...")

        # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        cr_path = self._get_model_path_base(KEY_CR)
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(cr_path, trust_remote_code=True)
            print(f"[INFO] í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ: {cr_path}")
        except Exception as e:
            print(f"[WARN] í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ : {e}. ì›ë³¸ ëª¨ë¸ ({MODEL_NAME}) ë¡œë“œ ì‹œë„.")
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
            print(f"[INFO] ê¸°ë³¸ ëª¨ë¸ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ.")

        # 2. ONNX InferenceSession ë¡œë“œ
        options = SessionOptions()
        providers = ['CPUExecutionProvider']

        for judge_type in JUDGE_TYPES:
            try:
                # model_folder_pathëŠ” PyTorchì™€ ë™ì¼í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                model_folder_path = self._get_model_path_base(judge_type)
                # onnx íŒŒì¼ì€ ëª¨ë¸ í´ë”ì— ê°™ì´ ë“¤ì–´ìˆìŒ
                quant_model_path = os.path.join(model_folder_path, "model_quant.onnx")
                if not os.path.exists(quant_model_path):
                    raise FileNotFoundError(f"ONNX íŒŒì¼ ì—†ìŒ. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ë¡œ : {quant_model_path}")

                session = InferenceSession(quant_model_path, sess_options=options, providers=providers)
                self.judges[judge_type] = session
                print(f"[SUCCESS] {judge_type} Judge ë¡œë“œ ì™„ë£Œ (ONNX)")
            except Exception as e:
                print(f"[ERROR] {judge_type} ë¡œë“œ ì‹¤íŒ¨: {e}")

        if len(self.judges) != len(JUDGE_TYPES):
            raise RuntimeError(f"í•„ìš”í•œ ONNX ëª¨ë¸ {len(JUDGE_TYPES)}ê°œ ì¤‘ {len(self.judges)}ê°œë§Œ ë¡œë“œë¨.")

        print(f">> ARES ì‹¬ì‚¬ê´€ ë¡œë“œ ì™„ë£Œ. ì´ {len(self.judges)}ê°œ ì‹¬ì‚¬ê´€ í™œì„±í™”.")

    def _get_logits(self, judge_type: str, inputs: Dict[str, Any], tokenized_data: Dict[str, Any]) -> torch.Tensor:
        """ONNX ì¶”ë¡  ë¡œì§: Logitsë¥¼ NumPyë¡œ ì–»ì€ í›„ PyTorch Tensorë¡œ ë³€í™˜."""
        session: InferenceSession = self.judges[judge_type]

        # ONNX ì¶”ë¡  (NumPy í…ì„œ ì‚¬ìš©)
        input_names = [input.name for input in session.get_inputs()]

        # tokenized_data (NumPy ë°°ì—´)ì—ì„œ ONNX ì„¸ì…˜ì´ í•„ìš”ë¡œ í•˜ëŠ” ì…ë ¥ë§Œ ì¶”ì¶œ
        session_input = {
            name: tokenized_data[name] for name in input_names if name in tokenized_data
        }

        outputs = session.run(None, session_input)
        logits_array = outputs[0]

        # Logitsë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜í•˜ì—¬ ìŠˆí¼ í´ë˜ìŠ¤ì˜ í›„ì²˜ë¦¬ì— ì‚¬ìš©
        return torch.from_numpy(logits_array)

    # evaluate_triple ë©”ì„œë“œëŠ” AresJudge ìŠˆí¼ í´ë˜ìŠ¤ì—ì„œ ìƒì† ë° ì¬í™œìš©ë¨


# ===================================================================
# 3. PPI í†µê³„ ë° ë³´ì • ê³„ì‚° í´ë˜ìŠ¤
# ===================================================================
# 02_generate_ppi.py (PPICalculator í´ë˜ìŠ¤ ë‚´ë¶€)

class PPICalculator:
    """PPI ë³´ì • ë° í†µê³„ ê³„ì‚°ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤."""

    def __init__(self, ci_z_score: float = CI_Z_SCORE):
        self.ci_z_score = ci_z_score

    def calculate_accuracy(self, stats: Dict[str, Any]) -> float | str:
        """rectifier_termsë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¬ì‚¬ê´€ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³  float ë˜ëŠ” 'N/A'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        terms = stats.get('rectifier_terms')
        labeled_n = stats.get('labeled_n')

        if not terms or labeled_n == 0:
            return 'N/A'

        # ì˜¤ë¥˜ íšŸìˆ˜ = (rectifier_terms ë¦¬ìŠ¤íŠ¸ì—ì„œ 0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜)
        error_count = sum(1 for term in terms if term != 0.0)
        accuracy = 1.0 - (error_count / labeled_n)
        return accuracy

    def generate_golden_set_stat(self, golden_set_stats: Dict[str, Dict[str, Any]]) -> Dict[
        str, Dict[str, float | str]]:
        """
        ê³¨ë“ ì…‹ í†µê³„ ê²°ê³¼ë¥¼ ìˆœìˆ˜í•œ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        (report_utilì—ì„œ MD í¬ë§·íŒ… ë‹´ë‹¹)
        """
        if not golden_set_stats:
            print("[FAIL] ê³¨ë“ ì…‹ í‰ê°€ ê²°ê³¼ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return {}

        report_data = {}

        for axis in JUDGE_TYPES:
            stats = golden_set_stats.get(axis, {})

            # LM ì˜ˆì¸¡ í‰ê·  ê°’
            mean_pred = stats.get('machine_mean', 'N/A')

            # ì‹¬ì‚¬ê´€ ì •í™•ë„ ê°’
            accuracy = self.calculate_accuracy(stats)

            report_data[axis] = {
                'mean_pred': mean_pred,  # LM ì˜ˆì¸¡ í‰ê·  (float | 'N/A')
                'accuracy': accuracy  # ì‹¬ì‚¬ê´€ ì •í™•ë„ (float | 'N/A')
            }

        return report_data

    def evaluate_golden_set(self, judge: AresJudge, golden_set_filepath: str) -> Dict[str, Dict[str, Any]]:
        """
        íŠ¹ì • ê³¨ë“ ì…‹ íŒŒì¼ì„ ì‹¬ì‚¬ê´€ì´ í‰ê°€í•˜ê³ , PPI í¸í–¥ ê³„ì‚°ì— í•„ìš”í•œ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        ë°˜í™˜ ë”•ì…”ë„ˆë¦¬ì— 'prob_pos_list' (ê³¨ë“ ì…‹ì˜ P_pos ë¦¬ìŠ¤íŠ¸)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        golden_stats = {key: {
            'labeled_n': 0,
            'rectifier_terms': [],
            'machine_mean_sum': 0.0,
            'prob_pos_list': []  # ğŸš¨ ì¶”ê°€: ê³¨ë“ ì…‹ì˜ P_pos ë¦¬ìŠ¤íŠ¸
        } for key in JUDGE_TYPES}

        golden_records = _load_json_lines(golden_set_filepath)

        if not golden_records:
            print(f"[WARN] ê³¨ë“ ì…‹ íŒŒì¼ {golden_set_filepath}ì— í‰ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        total_samples = len(golden_records)
        print(f"\n>> ê³¨ë“ ì…‹ í‰ê°€ ì‹œì‘. {golden_set_filepath}, ì´ {total_samples}ê°œ ì˜ˆì¸¡ ì‹œì‘ ")

        progress_points = [0, 10, 30, 50, 70, 90, 100]
        next_progress_index = 0

        print(f"[PROGRESS] 0%. ", end=' ')

        for i, data in enumerate(golden_records):
            current_progress = (i * 100) // total_samples

            # ë‹¤ìŒ ëª©í‘œ %ì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸
            if next_progress_index < len(progress_points) - 1 and current_progress >= progress_points[
                next_progress_index + 1]:
                next_progress_index += 1
                print(f" {progress_points[next_progress_index]}%. ", end=' ')

            try:
                # Q, C, A ì¶”ì¶œ ë° ì •ê·œí™”
                query = ' '.join(data.get('q', '').split()).strip()
                context = ' '.join(data.get('c', '').split()).strip()
                answer = ' '.join(data.get('a', '').split()).strip()

                if not all([query, context, answer]):
                    continue

                scores_with_probs = judge.evaluate_triple(query, context, answer)

                for axis in JUDGE_TYPES:
                    pred_key = JUDGE_PREDICTION_FIELDS[axis]
                    axis_scores = scores_with_probs.get(pred_key)

                    if axis_scores is None: continue

                    machine_pred = axis_scores.get('machine_pred')
                    prob_pos = axis_scores.get('prob_pos')

                    gold_key = GOLD_LABEL_FIELDS[axis]
                    gold_label = data.get(gold_key)

                    if machine_pred is None or prob_pos is None or gold_label is None:
                        continue

                    # í†µê³„ ì—…ë°ì´íŠ¸
                    machine_pred = int(machine_pred)
                    gold_label = int(gold_label)

                    rectifier_term = float(machine_pred - gold_label)

                    stats = golden_stats[axis]
                    stats['labeled_n'] += 1
                    stats['rectifier_terms'].append(rectifier_term)
                    stats['machine_mean_sum'] += machine_pred
                    stats['prob_pos_list'].append(prob_pos)

            except Exception as e:
                print(f"[ì˜ˆì™¸] {e}")
                continue

        # 100% ì™„ë£Œ ë¡œê·¸ ì¶œë ¥ (ë£¨í”„ ë‚´ì—ì„œ ì´ë¯¸ ì¶œë ¥ë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë‚˜, ì•ˆì „ì¥ì¹˜)
        if total_samples > 0:
            print(f"100% ì™„ë£Œ.")

        # 3. ìµœì¢… í†µê³„ ê³„ì‚°
        final_golden_stats = {}
        for axis, stats in golden_stats.items():
            if stats['labeled_n'] > 0:
                final_golden_stats[axis] = {
                    'labeled_n': stats['labeled_n'],
                    'rectifier_terms': stats['rectifier_terms'],
                    'machine_mean': stats['machine_mean_sum'] / stats['labeled_n'],
                    'prob_pos_list': stats['prob_pos_list']
                }

        return final_golden_stats


    def calculate_ppi_asymptotic_ci(
            self,
            machine_preds: List[int],
            rectifiers: List[float],
            total_n: int,
            labeled_n: int
    ) -> float:
        """PPI Asymptotic CI Half-width ê³„ì‚°."""
        if labeled_n <= 1 or total_n <= 0:
            return 0.0

        y_hat_array: NDArray[np.float64] = np.array(machine_preds, dtype=np.float64)
        rectifier_array: NDArray[np.float64] = np.array(rectifiers, dtype=np.float64)

        sigma2_f: float = float(np.var(y_hat_array))
        sigma2_rec: float = float(np.var(rectifier_array))

        variance: float = (sigma2_f / float(total_n)) + (sigma2_rec / float(labeled_n))
        half_width: float = self.ci_z_score * math.sqrt(variance)
        return round(half_width, 3)

    def _calculate_binned_probs(self, probs_pos: List[float], total_n: int) -> List[Dict[str, float | int | str]]:
        """Softmax P_pos ê°’ì„ 10ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¹ˆë„ì™€ ë¹„ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        probs_array = np.array(probs_pos)
        # 0.0 to 1.0, 10 bins. range=(0.0, 1.000001)ì„ ì‚¬ìš©í•˜ì—¬ 1.0ì„ í¬í•¨í•©ë‹ˆë‹¤.
        counts, _ = np.histogram(probs_array, bins=10, range=(0.0, 1.000001))

        binned_data = []

        for i in range(10):
            low = i * 0.1
            high = (i + 1) * 0.1
            count = int(counts[i])
            percentage = round((count / total_n) * 100, 1) if total_n > 0 else 0.0

            binned_data.append({
                'range': f"{low:.1f} - {high:.1f}",
                'count': count,
                'percentage': percentage
            })

        # ì‹œê°í™”ë¥¼ ìœ„í•´ ìµœëŒ€ ë¹„ìœ¨ì„ ê³„ì‚° (report_utilì—ì„œ ë§‰ëŒ€ ê¸¸ì´ ì •ê·œí™”ìš©)
        max_perc = max(item['percentage'] for item in binned_data)

        # ëª¨ë“  ë¹ˆì— max_perc ê°’ì„ ì¶”ê°€
        for item in binned_data:
            item['max_perc'] = max_perc

        return binned_data

    def _calculate_confidence_stats(self, scores: List[int], probs_pos: List[float]) -> Dict[str, float]:
        """Softmax ê¸ì • í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ í™•ì‹ ë„ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (ì¤‘ì•™ê°’ í¬í•¨)"""

        scores_array = np.array(scores)
        probs_pos_array = np.array(probs_pos)
        probs_neg_array = 1.0 - probs_pos_array

        # 1. ê¸°ë³¸ ë¶„í¬ í†µê³„ ê³„ì‚° (P_pos ë° P_neg ê¸°ì¤€)
        stats = {
            'prob_pos_min': round(float(np.min(probs_pos_array)), 4),
            'prob_pos_avg': round(float(np.mean(probs_pos_array)), 4),
            'prob_pos_median': round(float(np.median(probs_pos_array)), 4),
            'prob_pos_max': round(float(np.max(probs_pos_array)), 4),

            'prob_neg_min': round(float(np.min(probs_neg_array)), 4),
            'prob_neg_avg': round(float(np.mean(probs_neg_array)), 4),
            'prob_neg_median': round(float(np.median(probs_neg_array)), 4),
            'prob_neg_max': round(float(np.max(probs_neg_array)), 4),
        }

        # 2. ì¡°ê±´ë¶€ í™•ì‹ ë„ ë° ë§ˆì§„ ê³„ì‚° (íŒŒìƒ ì§€í‘œ)

        margin_array = np.abs(probs_pos_array - probs_neg_array)
        stats['mean_margin'] = round(float(np.mean(margin_array)), 4)

        pos_mask = scores_array == 1
        num_pos_preds = np.sum(pos_mask)
        neg_mask = scores_array == 0
        num_neg_preds = np.sum(neg_mask)

        stats['conf_pos_avg'] = round(float(np.mean(probs_pos_array[pos_mask])), 4) if num_pos_preds > 0 else 0.0
        stats['conf_neg_avg'] = round(float(np.mean(probs_neg_array[neg_mask])), 4) if num_neg_preds > 0 else 0.0

        return stats


    def calculate_ppi_summary(
            self,
            file_base_name: str,
            current_lm_scores: Dict[str, List[int]],
            current_lm_probs: Dict[str, List[float]],
            total_n: int,
            golden_set_stats: Dict[str, Dict[str, Any]],
            golden_set_name: str
    ) -> Dict[str, Any]:
        """
        LM ì˜ˆì¸¡ ê²°ê³¼ì™€ ê³¨ë“ ì…‹ í†µê³„ë¥¼ ê²°í•©í•˜ì—¬ PPI ìš”ì•½ ê²°ê³¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        (í™•ì‹ ë„ í†µê³„, í‰ê°€ì…‹ Binning ë°ì´í„°, ê³¨ë“ ì…‹ Binning ë°ì´í„°ë¥¼ í¬í•¨)
        """

        summary: Dict[str, Any] = {
            "model_name": file_base_name,
            "golden_set_name": golden_set_name,
            "n": total_n,
            "ppi_active": True,
            "labeled_n_rep": golden_set_stats.get(KEY_CR, {}).get('labeled_n', 0),
        }

        overall_corrected_scores: List[float] = []

        for axis in JUDGE_TYPES:
            scores: List[int] = current_lm_scores[axis]
            probs_pos: List[float] = current_lm_probs[axis]

            golden_axis_stats = golden_set_stats.get(axis, {})
            labeled_n_axis: int = golden_axis_stats.get('labeled_n', 0)

            # í•„ìˆ˜ í†µê³„ ëˆ„ë½ ê²€ì‚¬ (ì´ì „ ë‹¨ê³„ ìˆ˜ì • ë¡œì§ ìœ ì§€)
            if labeled_n_axis == 0 or not golden_axis_stats:
                raise RuntimeError(
                    f"[{axis} ì¶• í†µê³„ ëˆ„ë½] ê³¨ë“ ì…‹ '{golden_set_name}'ì— '{axis}' ì¶•ì˜ ìœ íš¨í•œ ë¼ë²¨ë§ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. "
                    f"Labeled N: {labeled_n_axis}."
                )

            rectifier_terms: List[float] = golden_axis_stats['rectifier_terms']

            # 1. PPI í†µê³„ ê³„ì‚°
            machine_mean: float = sum(scores) / float(total_n)
            rectifier: float = sum(rectifier_terms) / labeled_n_axis
            corrected_mean: float = max(0.0, min(1.0, machine_mean - rectifier))
            margin: float = self.calculate_ppi_asymptotic_ci(scores, rectifier_terms, total_n, labeled_n_axis)

            # 2. í™•ì‹ ë„ í†µê³„ ê³„ì‚° (ê¸°ìˆ  í†µê³„ëŸ‰)
            confidence_stats = self._calculate_confidence_stats(scores, probs_pos)

            # 3. Binning ë°ì´í„° ê³„ì‚°
            # í‰ê°€ ëŒ€ìƒì…‹ Binning
            prob_bins = self._calculate_binned_probs(probs_pos, total_n)

            # ğŸš¨ ì¶”ê°€: ê³¨ë“ ì…‹ Binning ë°ì´í„° ê³„ì‚°
            golden_probs_pos: List[float] = golden_axis_stats['prob_pos_list']
            # labeled_n_axisëŠ” ê³¨ë“ ì…‹ì˜ ì´ ìƒ˜í”Œ ìˆ˜(N) ì—­í• ì„ í•©ë‹ˆë‹¤.
            golden_prob_bins = self._calculate_binned_probs(golden_probs_pos, labeled_n_axis)

            # 4. ìµœì¢… ìš”ì•½ì— ì¶”ê°€
            summary[axis] = {
                # PPI í†µê³„
                "machine_mean": round(machine_mean, 2),
                "corrected_mean": round(corrected_mean, 2),
                "applied_rectifier": round(rectifier, 3),
                "ci": round(margin, 2),
                # í™•ì‹ ë„ í†µê³„ í•©ë³‘
                **confidence_stats,
                # Softmax Binning ë°ì´í„°
                'prob_bins': prob_bins,  # í‰ê°€ì…‹ íˆìŠ¤í† ê·¸ë¨ìš©
                'golden_prob_bins': golden_prob_bins  # ğŸš¨ ì¶”ê°€: ê³¨ë“ ì…‹ íˆìŠ¤í† ê·¸ë¨ìš©
            }
            overall_corrected_scores.append(corrected_mean)

        if overall_corrected_scores:
            summary["overall"] = round(sum(overall_corrected_scores) / len(overall_corrected_scores), 2)

        return summary


# ===================================================================
# 4. ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜
# ===================================================================
def _load_judges_and_calc() -> Tuple[AresJudge, PPICalculator]:
    """ì‹¬ì‚¬ê´€ê³¼ ê³„ì‚°ê¸° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë¡œë“œí•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""

    lower_model_name = MODEL_NAME.lower()
    if "onnx" in lower_model_name and "sbert" in lower_model_name:
        # SBERT ONNX ëª¨ë¸ì¸ ê²½ìš°
        judge = AresSbertOnnxJudge(DEVICE, MAX_LENGTH, model_type_name="SBert-Onnx")
    elif "distil" in lower_model_name:
        # DistilBERT ëª¨ë¸ì¸ ê²½ìš°
        judge = AresPytorchJudge(DEVICE, MAX_LENGTH, model_type_name="DistilBERT")
    else:
        # ì¼ë°˜ BERT ëª¨ë¸ì¸ ê²½ìš°
        judge = AresPytorchJudge(DEVICE, MAX_LENGTH, model_type_name="BERT")

    calculator = PPICalculator()
    return judge, calculator

# ê³¨ë“ ì…‹ í‰ê°€
def _evaluate_golden_sets(judge: AresJudge, calculator: PPICalculator) -> Tuple[Dict, Dict]:
    """
    ê³¨ë“ ì…‹ ë””ë ‰í† ë¦¬ë¥¼ íƒìƒ‰í•˜ê³ , ê° ê³¨ë“ ì…‹ì„ í‰ê°€í•˜ì—¬ í†µê³„ ë§µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ê³¨ë“ ì…‹ í†µê³„ê°€ ì—†ìœ¼ë©´ Fatal Error ë°œìƒ)
    """
    GOLDEN_DIR = config.DATA_GOLDEN_DIR

    # 1. ê³¨ë“  ë¼ë²¨ íŒŒì¼ ê²€ìƒ‰
    golden_files: List[Tuple[str, str]] = []
    if os.path.isdir(GOLDEN_DIR):
        for filename in os.listdir(GOLDEN_DIR):
            if filename.endswith('.jsonl'):
                golden_name = filename.replace('.jsonl', '')
                file_path = os.path.join(GOLDEN_DIR, filename)
                golden_files.append((golden_name, file_path))

        print(f"\n[INFO] {GOLDEN_DIR}ì—ì„œ ì´ {len(golden_files)}ê°œì˜ ê³¨ë“ ì…‹ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    else:
        print(f"\n[WARN] ê³¨ë“ ì…‹ ë””ë ‰í† ë¦¬ '{GOLDEN_DIR}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    if not golden_files:
        raise RuntimeError("PPI ë³´ì •ì„ ìœ„í•œ ê³¨ë“  ë¼ë²¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # 2. ëª¨ë“  ê³¨ë“ ì…‹ í‰ê°€ ë° í†µê³„ ì €ì¥
    golden_stats_map: Dict[str, Dict[str, Dict[str, Any]]] = {}
    # ğŸš¨ ìˆ˜ì •: ìˆœìˆ˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ë¡œ ë³€ê²½ (Markdown ì•„ë‹˜)
    golden_report_data: Dict[str, Dict[str, Dict[str, float | str]]] = {}

    for golden_name, path in golden_files:
        try:
            stats = calculator.evaluate_golden_set(judge, path)
            if stats:
                golden_stats_map[golden_name] = stats

                # ğŸš¨ ìˆ˜ì •: ìˆœìˆ˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ ë° ì €ì¥
                report_data = calculator.generate_golden_set_stat(stats)
                golden_report_data[golden_name] = report_data

                print(f"[SUCCESS] ê³¨ë“ ì…‹ '{golden_name}' í‰ê°€ ì™„ë£Œ.")
            else:
                print(f"\n   [WARN] ê³¨ë“ ì…‹ '{golden_name}' í‰ê°€ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            print(f"\n[FATAL ERROR] ê³¨ë“ ì…‹ '{golden_name}' í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê±´ë„ˆëœë‹ˆë‹¤.")

    if not golden_stats_map:
        raise RuntimeError("PPI ë³´ì •ì„ ìœ„í•œ ìœ íš¨í•œ ê³¨ë“  ë¼ë²¨ ë°ì´í„° í‰ê°€ ì‹¤íŒ¨.")

    # ğŸš¨ ìˆ˜ì •: golden_report_data ë°˜í™˜
    return golden_stats_map, golden_report_data


# ëŒ€ê·œëª¨ í‰ê°€ ë£¨í”„
def _process_input_files(judge: AresJudge, calculator: PPICalculator, golden_stats_map: Dict) -> Tuple[
    List, int, float]:
    """
    ì…ë ¥ íŒŒì¼ì„ ìˆœíšŒí•˜ë©° LM ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , PPI ë³´ì •ì„ ì ìš©í•˜ì—¬ model_summariesë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    INPUT_DIR = config.DATA_IN_DIR
    input_files = [
        os.path.join(INPUT_DIR, f) for f in os.listdir(INPUT_DIR) if f.endswith('.jsonl')
    ]

    if not input_files:
        print(f"\n[WARN] QCA `.jsonl` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return [], 0, 0.0

    print(f"\n[INFO] í‰ê°€ ëŒ€ìƒ íŒŒì¼ ê°¯ìˆ˜ : {len(input_files)}")

    total_successful_evals = 0
    model_summaries: List[Dict[str, Any]] = []
    full_start_time = time.time()

    # 2-2. íŒŒì¼ë³„ í‰ê°€, LM ì˜ˆì¸¡ íŒŒì¼ ìƒì„± ë° ê²°ê³¼ ì§‘ê³„
    for file_path in input_files:
        start_time = time.time()
        file_base_name = os.path.basename(file_path).replace('.jsonl', '')

        # QCA í‰ê°€ (Judge)ëŠ” íŒŒì¼ ë‹¹ í•œ ë²ˆë§Œ ìˆ˜í–‰
        print(f"\n--- í‰ê°€ ì‹œì‘: {file_base_name} ---")
        current_lm_scores = {k: [] for k in JUDGE_TYPES}
        current_lm_probs = {k: [] for k in JUDGE_TYPES}  # ğŸš¨ ìˆ˜ì •: Softmax ê¸ì • í™•ë¥  ì§‘ê³„ìš©
        # all_results_for_file = [] # ì €ì¥ ë¡œì§ ë¹„í™œì„±í™” ì‹œ ë¶ˆí•„ìš”
        processed_count_in_file = 0

        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

            total_samples = len(lines)
            progress_points = [0, 10, 30, 50, 70, 90, 100]
            next_progress_index = 0

            if total_samples > 0:
                print(f"[PROGRESS] 0%. ", end=' ')

            for i, line in enumerate(lines):
                current_progress = (i * 100) // total_samples

                if next_progress_index < len(progress_points) - 1 and current_progress >= progress_points[
                    next_progress_index + 1]:
                    next_progress_index += 1
                    print(f"{progress_points[next_progress_index]}%  ", end=' ')

                try:
                    data = json.loads(line.strip())
                    query = ' '.join(data.get('q', '').split()).strip()
                    context = ' '.join(data.get('c', '').split()).strip()
                    answer = ' '.join(data.get('a', '').split()).strip()

                    if not all([query, context, answer]): continue

                    scores_with_probs = judge.evaluate_triple(query, context, answer)

                    for axis in JUDGE_TYPES:
                        pred_key = JUDGE_PREDICTION_FIELDS[axis]
                        axis_scores = scores_with_probs.get(pred_key)

                        if axis_scores is None: continue

                        current_lm_scores[axis].append(axis_scores.get('machine_pred', 0))
                        current_lm_probs[axis].append(axis_scores.get('prob_pos', 0.0))

                    total_successful_evals += 1
                    processed_count_in_file += 1

                except Exception as e:
                    print(f"[ERROR] í‰ê°€ ì¤‘ ì˜ˆì™¸ë°œìƒ : {e}")
                    continue

            if processed_count_in_file > 0:
                print(f"100% ì™„ë£Œ.")  # 100% ì™„ë£Œ ëª…ì‹œ

            end_time = time.time()
            print(f"[INFO] í‰ê°€ ì†Œìš”ì‹œê°„ : {end_time - start_time:,.2f}ì´ˆ ")

            # 2-3. í‰ê°€ì…‹ë‹¹ ëª¨ë“  ê³¨ë“ ì…‹ì— ëŒ€í•´ PPI í†µê³„ ê³„ì‚° ë° ì§‘ê³„
            if processed_count_in_file > 0:
                for golden_name, golden_stats in golden_stats_map.items():
                    summary = calculator.calculate_ppi_summary(
                        file_base_name,
                        current_lm_scores,
                        current_lm_probs,
                        processed_count_in_file,
                        golden_stats,
                        golden_name
                    )
                    model_summaries.append(summary)

                print(f"[ì§‘ê³„ ì™„ë£Œ] '{file_base_name}' ê²°ê³¼ ì§‘ê³„ ì™„ë£Œ. ({len(golden_stats_map)}ê°œ ê³¨ë“ ì…‹ ì ìš©)")
            else:
                print(f"[ERROR] ì‹¬ì‚¬ê´€ì˜ í‰ê°€ ê²°ê³¼ ì—†ìŒ - íŒŒì¼: {file_base_name}")

        full_elapsed_time = time.time() - full_start_time
        return model_summaries, total_successful_evals, full_elapsed_time


# ë³´ê³ ì„œ ë° ìš”ì•½
def _generate_report_and_summary(golden_markdown_map: Dict, model_summaries: List, total_successful_evals: int,
                                 full_elapsed_time: float) -> None:
    """
    ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ì½˜ì†”ì— ì‹¤í–‰ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    REPORT_DIR = config.DATA_REPORT_DIR
    MODEL_NAME = config.MODEL_NAME

    if model_summaries:
        report_content: str = report_util.generate_summary_report(golden_markdown_map, model_summaries)
        timestamp: str = time.strftime("%Y%m%d_%H%M%S")

        # íŒŒì¼ëª… ìˆ˜ì • ë°˜ì˜
        report_filename: str = f"{MODEL_NAME}_{timestamp}.md"
        output_path: str = os.path.join(REPORT_DIR, report_filename)

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        print(f"\n[ìµœì¢… ì™„ë£Œ] ARES í†µê³„ ë³´ê³ ì„œ ìƒì„±ë¨ â†’ {output_path}")

    print("\n\n=============== LM ì˜ˆì¸¡ ìƒì„± ìµœì¢… ìš”ì•½ ===============")
    print(f"ì´ LM ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: {total_successful_evals}ê°œ")
    print(f"ì´ ì†Œìš” ì‹œê°„: {full_elapsed_time:.2f}ì´ˆ")
    print("==================================================")



# ì‹¤í–‰
# 02_generate_ppi.py (run_ares_pipeline í•¨ìˆ˜)

def run_ares_pipeline():
    """
    ARES ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ê´€ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    """
    try:
        # 0. í™˜ê²½ ì„¤ì •
        os.makedirs(config.DATA_IN_DIR, exist_ok=True)
        os.makedirs(config.DATA_REPORT_DIR, exist_ok=True)
        print(f"\n[SETUP] í‰ê°€ëŒ€ìƒì¸ QCA ì…ë ¥ ë””ë ‰í† ë¦¬: {config.DATA_IN_DIR}, ë³´ê³ ì„œ ë””ë ‰í† ë¦¬: {config.DATA_REPORT_DIR}")

        # 1. ì´ˆê¸°í™” ë° ê³¨ë“ ì…‹ í‰ê°€
        current_judge, calculator = _load_judges_and_calc()
        # ğŸš¨ ìˆ˜ì •: golden_markdown_map ëŒ€ì‹  golden_report_data ë³€ìˆ˜ëª… ì‚¬ìš©
        golden_stats_map, golden_report_data = _evaluate_golden_sets(current_judge, calculator)

        # 2. ëŒ€ê·œëª¨ í‰ê°€ ì‹¤í–‰
        model_summaries, total_successful_evals, full_elapsed_time = _process_input_files(
            current_judge, calculator, golden_stats_map
        )

        # 3. ë³´ê³ ì„œ ìƒì„± ë° ìµœì¢… ìš”ì•½
        # ğŸš¨ ìˆ˜ì •: golden_report_data ì¸ìˆ˜ë¡œ ì „ë‹¬ (report_utilì—ì„œ MD ìƒì„±)
        _generate_report_and_summary(golden_report_data, model_summaries, total_successful_evals, full_elapsed_time)

    except RuntimeError as e:
        print(f"\n[FATAL ERROR] íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

if __name__ == "__main__":
    run_ares_pipeline()

