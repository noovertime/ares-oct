# 02_generate_ppi.py (í´ë˜ìŠ¤ ê¸°ë°˜ ë° ë‹¤ì¤‘ ê³¨ë“ ì…‹ í™•ì¥ ë²„ì „)
# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import json
import logging
import math
import os
import time
from typing import Dict, Any, List, Tuple

# 2. ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
from numpy.typing import NDArray
import torch
from scipy.stats import norm
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# 3. ë¡œì»¬/ì• í”Œë¦¬ì¼€ì´ì…˜ ê³ ìœ  ë¼ì´ë¸ŒëŸ¬ë¦¬
import config
import report_util
from config import (
    KEY_CR,
    KEY_AF,
    KEY_AR,
    JUDGE_TYPES,
    JUDGE_PREDICTION_FIELDS,
    GOLD_LABEL_FIELDS,
    TOKEN_TYPE_ID_OFF
)
from json_util import _load_json_lines  # ì™¸ë¶€ ëª¨ë“ˆ ê°€ì •

# ===================================================================
# 0. ì „ì—­ ìƒìˆ˜ ë° í™˜ê²½ ì„¤ì •
# ===================================================================

# Transformers ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™”
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)
logging.getLogger("transformers.modeling_utils").setLevel(logging.ERROR)

# ê²½ë¡œ ì„¤ì •
MODEL_DIR_BASE = config.MODEL_DIR
MODEL_NAME = config.MODEL_NAME

# CPU í™˜ê²½ ì„¤ì •
DEVICE = torch.device("cpu")
MAX_LENGTH = 128

# ARES ì‹¬ì‚¬ê´€ íƒ€ì… ë° í´ë” ë§¤í•‘
FOLDER_MAPPING = {
    config.KEY_CR: 'context_relevance',
    config.KEY_AF: 'answer_faithfulness',
    config.KEY_AR: 'answer_relevance'
}

CI_ALPHA: float = 0.05  # 95% ì‹ ë¢°êµ¬ê°„
CI_Z_SCORE: float = float(norm.ppf(1 - CI_ALPHA / 2))  # ì•½ 1.96


# ===================================================================
# 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (í´ë˜ìŠ¤ì— í¬í•¨ì‹œí‚¤ì§€ ì•ŠëŠ” ë²”ìš© ê¸°ëŠ¥)
# ===================================================================

def _find_model_path(judge_type: str) -> str:
    """ê³ ì •ëœ ì‹¬ì‚¬ê´€ ì´ë¦„ í´ë” ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # judge_typeì€ KEY_CR, KEY_AF, KEY_AR ì¤‘ í•˜ë‚˜
    target_folder = FOLDER_MAPPING.get(judge_type)

    if not target_folder:
        raise ValueError(f"ì •ì˜ë˜ì§€ ì•Šì€ ì‹¬ì‚¬ê´€ íƒ€ì…: {judge_type}")

    model_path = os.path.join(MODEL_DIR_BASE, target_folder)

    if not os.path.isdir(model_path):
        raise FileNotFoundError(f"ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    return model_path


def cleanup_evaluation_data():
    """
    configì— ì •ì˜ëœ PPI ì¶œë ¥ ë””ë ‰í† ë¦¬ì™€ ìµœì¢… ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    dirs_to_clean = [ config.DATA_REPORT_DIR]
    print("\n>> í‰ê°€ ë° ë³´ê³ ì„œ ì¶œë ¥ íŒŒì¼ ì •ë¦¬ ì‹œì‘...")
    for target_dir in dirs_to_clean:
        if os.path.isdir(target_dir):
            files_deleted = 0
            # ... (ê¸°ì¡´ cleanup_evaluation_data ë¡œì§ ìœ ì§€)
            for filename in os.listdir(target_dir):
                file_path = os.path.join(target_dir, filename)
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        files_deleted += 1
                except Exception as e:
                    print(f"   [ERROR] íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")

            print(f"   [SUCCESS] '{target_dir}' ë””ë ‰í† ë¦¬ì—ì„œ ì´ {files_deleted}ê°œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ.")
        else:
            print(f"   [INFO] ë””ë ‰í† ë¦¬ '{target_dir}'ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì •ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")


# ===================================================================
# 2. ARES í‰ê°€ í´ë˜ìŠ¤
# ===================================================================
class AresJudge:
    """ARES ì‹¬ì‚¬ê´€(í† í¬ë‚˜ì´ì € ë° 3ê°œ ëª¨ë¸) ë¡œë”© ë° í‰ê°€ ë‹´ë‹¹ í´ë˜ìŠ¤."""

    def __init__(self, device: torch.device = DEVICE, max_length: int = MAX_LENGTH) -> None:
        self.tokenizer: AutoTokenizer | None = None
        self.judges: Dict[str, AutoModelForSequenceClassification] = {}
        self.device = device
        self.max_length = max_length
        self._load_models()

    def _load_models(self) -> None:
        """CR, AF, AR ì„¸ ê°€ì§€ ì‹¬ì‚¬ê´€ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ."""
        print("\n>> ARES ì‹¬ì‚¬ê´€ ë¡œë”© ì‹œì‘...")

        # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
        cr_path = _find_model_path(KEY_CR)
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(cr_path, trust_remote_code=True)
            print(f"[INFO] í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ: {cr_path}")
        except Exception:
            print(f"[WARN] {cr_path} ì—ì„œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸({MODEL_NAME}) ë¡œë“œ ì‹œë„")
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
            print(f"[INFO] ê¸°ë³¸ ëª¨ë¸ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")

        # DistilBERT í˜¸í™˜: token_type_ids ì œê±°
        if TOKEN_TYPE_ID_OFF and self.tokenizer:
            self.tokenizer.model_input_names = [
                n for n in self.tokenizer.model_input_names if n != "token_type_ids"
            ]
            print("[INFO] 'token_type_ids' ë¹„í™œì„±í™” ì™„ë£Œ")

        # 2. ëª¨ë¸ ë¡œë“œ
        for judge_type in JUDGE_TYPES:
            try:
                path = _find_model_path(judge_type)
                model = AutoModelForSequenceClassification.from_pretrained(
                    path, num_labels=2, trust_remote_code=True
                ).to(self.device)
                model.eval()
                self.judges[judge_type] = model
                print(f"[SUCCESS] {judge_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({path})")
            except Exception as e:
                print(f"[ERROR] {judge_type} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

        if len(self.judges) != 3:
            raise RuntimeError(
                f"í•„ìš”í•œ ëª¨ë¸ 3ê°œ ì¤‘ {len(self.judges)}ê°œë§Œ ë¡œë“œë¨ â€” ëª¨ë“  ì‹¬ì‚¬ê´€ í•„ìš”."
            )

        print(f">> ARES ì‹¬ì‚¬ê´€ ë¡œë“œ ì™„ë£Œ. ì´ {len(self.judges)}ê°œ ëª¨ë¸ í™œì„±í™”.")


    def evaluate_triple(self, query: str, context: str, answer: str) -> Dict[str, Dict[str, float | int]]:
        """
        í•˜ë‚˜ì˜ Q-C-A ìŒì— ëŒ€í•´ CR, AF, AR ì ìˆ˜(0 ë˜ëŠ” 1)ì™€ Softmax í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        ë°˜í™˜ í˜•ì‹ ë³€ê²½: { 'contextrelevance': {'machine_pred': 0/1, 'prob_neg': 0.xx, 'prob_pos': 0.yy}, ... }
        """
        # ë°˜í™˜ íƒ€ì…ì´ ë³€ê²½ë˜ë¯€ë¡œ ë”•ì…”ë„ˆë¦¬ì˜ ê°’ë„ ë”•ì…”ë„ˆë¦¬ê°€ ë©ë‹ˆë‹¤.
        results: Dict[str, Dict[str, float | int]] = {}

        judge_inputs = {
            JUDGE_PREDICTION_FIELDS[KEY_CR]: (query, context, self.judges[KEY_CR]),
            JUDGE_PREDICTION_FIELDS[KEY_AF]: (context, answer, self.judges[KEY_AF]),
            JUDGE_PREDICTION_FIELDS[KEY_AR]: (query, answer, self.judges[KEY_AR]),
        }

        with torch.no_grad():
            for name, (a, b, model) in judge_inputs.items():
                inputs = self.tokenizer(
                    a, b,
                    return_tensors="pt",
                    truncation=True,
                    padding="max_length",
                    max_length=self.max_length,
                ).to(self.device)

                outputs = model.forward(**inputs)
                logits = outputs.logits

                # 1. Softmax ì ìš©í•˜ì—¬ í™•ë¥  íšë“
                probabilities = torch.softmax(logits, dim=1).squeeze().cpu().numpy()

                # 2. í´ë˜ìŠ¤ ì˜ˆì¸¡ (argmax)
                prediction = int(torch.argmax(logits, dim=1).item())

                # 3. ê¸ì •(1) ë° ë¶€ì •(0) í™•ë¥  ì €ì¥ (ì†Œìˆ˜ì  4ì§¸ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼)
                prob_neg = round(float(probabilities[0]), 4)  # ë¶€ì • í™•ë¥  (Class 0)
                prob_pos = round(float(probabilities[1]), 4)  # ê¸ì • í™•ë¥  (Class 1)

                # 4. ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì— Softmax í™•ë¥ ê³¼ ì˜ˆì¸¡ í´ë˜ìŠ¤ ëª¨ë‘ ì €ì¥
                results[name] = {
                    'machine_pred': prediction,
                    'prob_neg': prob_neg,
                    'prob_pos': prob_pos
                }

        return results


# ===================================================================
# 3. PPI í†µê³„ ë° ë³´ì • ê³„ì‚° í´ë˜ìŠ¤
# ===================================================================
# 02_generate_ppi.py (PPICalculator í´ë˜ìŠ¤ ë‚´ë¶€)

class PPICalculator:
    """PPI ë³´ì • ë° í†µê³„ ê³„ì‚°ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤."""

    def __init__(self, ci_z_score: float = CI_Z_SCORE):
        self.ci_z_score = ci_z_score

    def calculate_accuracy(self, stats: Dict[str, Any]) -> float | str:
        """rectifier_termsë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¬ì‚¬ê´€ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³  float ë˜ëŠ” 'N/A'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        terms = stats.get('rectifier_terms')
        labeled_n = stats.get('labeled_n')

        if not terms or labeled_n == 0:
            return 'N/A'

        # ì˜¤ë¥˜ íšŸìˆ˜ = (rectifier_terms ë¦¬ìŠ¤íŠ¸ì—ì„œ 0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜)
        error_count = sum(1 for term in terms if term != 0.0)
        accuracy = 1.0 - (error_count / labeled_n)
        return accuracy

    def generate_golden_set_stat(self, golden_set_stats: Dict[str, Dict[str, Any]]) -> Dict[
        str, Dict[str, float | str]]:
        """
        ê³¨ë“ ì…‹ í†µê³„ ê²°ê³¼ë¥¼ ìˆœìˆ˜í•œ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        (report_utilì—ì„œ MD í¬ë§·íŒ… ë‹´ë‹¹)
        """
        if not golden_set_stats:
            print("[FAIL] ê³¨ë“ ì…‹ í‰ê°€ ê²°ê³¼ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return {}

        report_data = {}

        for axis in JUDGE_TYPES:
            stats = golden_set_stats.get(axis, {})

            # LM ì˜ˆì¸¡ í‰ê·  ê°’
            mean_pred = stats.get('machine_mean', 'N/A')

            # ì‹¬ì‚¬ê´€ ì •í™•ë„ ê°’
            accuracy = self.calculate_accuracy(stats)

            report_data[axis] = {
                'mean_pred': mean_pred,  # LM ì˜ˆì¸¡ í‰ê·  (float | 'N/A')
                'accuracy': accuracy  # ì‹¬ì‚¬ê´€ ì •í™•ë„ (float | 'N/A')
            }

        return report_data

    def evaluate_golden_set(self, judge: AresJudge, golden_set_filepath: str) -> Dict[str, Dict[str, Any]]:
        """
        íŠ¹ì • ê³¨ë“ ì…‹ íŒŒì¼ì„ ì‹¬ì‚¬ê´€ì´ í‰ê°€í•˜ê³ , PPI í¸í–¥ ê³„ì‚°ì— í•„ìš”í•œ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        ë°˜í™˜ ë”•ì…”ë„ˆë¦¬ì— 'prob_pos_list' (ê³¨ë“ ì…‹ì˜ P_pos ë¦¬ìŠ¤íŠ¸)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        golden_stats = {key: {
            'labeled_n': 0,
            'rectifier_terms': [],
            'machine_mean_sum': 0.0,
            'prob_pos_list': []  # ğŸš¨ ì¶”ê°€: ê³¨ë“ ì…‹ì˜ P_pos ë¦¬ìŠ¤íŠ¸
        } for key in JUDGE_TYPES}

        golden_records = _load_json_lines(golden_set_filepath)

        if not golden_records:
            print(f"[WARN] ê³¨ë“ ì…‹ íŒŒì¼ {golden_set_filepath}ì— í‰ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        print(f"\n>> ê³¨ë“ ì…‹ í‰ê°€ ì‹œì‘. {golden_set_filepath}, ì´ {len(golden_records)}ê°œ ìƒ˜í”Œ ì‹¬ì‚¬ê´€ ì˜ˆì¸¡ ì¤‘...")

        with tqdm(golden_records, desc="ê³¨ë“ ì…‹ ì‹¬ì‚¬ê´€ í‰ê°€ ì¤‘", leave=False) as pbar:
            for data in pbar:
                try:
                    # Q, C, A ì¶”ì¶œ ë° ì •ê·œí™”
                    query = ' '.join(data.get('q', '').split()).strip()
                    context = ' '.join(data.get('c', '').split()).strip()
                    answer = ' '.join(data.get('a', '').split()).strip()

                    if not all([query, context, answer]):
                        continue

                    # 1. LM ì‹¬ì‚¬ê´€ ì˜ˆì¸¡ (Yhat_labeled) - ë”•ì…”ë„ˆë¦¬ in ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜
                    scores_with_probs = judge.evaluate_triple(query, context, answer)

                    # 2. LM ì˜ˆì¸¡ê°’ê³¼ ì¸ê°„ ì£¼ì„ê°’ ë¹„êµ
                    for axis in JUDGE_TYPES:
                        pred_key = JUDGE_PREDICTION_FIELDS[axis]
                        axis_scores = scores_with_probs.get(pred_key)

                        if axis_scores is None: continue

                        machine_pred = axis_scores.get('machine_pred')
                        # ğŸš¨ ì¶”ê°€: P_pos í™•ë¥  ì¶”ì¶œ
                        prob_pos = axis_scores.get('prob_pos')

                        gold_key = GOLD_LABEL_FIELDS[axis]
                        gold_label = data.get(gold_key)

                        if machine_pred is None or prob_pos is None or gold_label is None:
                            continue

                        # í†µê³„ ì—…ë°ì´íŠ¸
                        machine_pred = int(machine_pred)
                        gold_label = int(gold_label)

                        rectifier_term = float(machine_pred - gold_label)

                        stats = golden_stats[axis]
                        stats['labeled_n'] += 1
                        stats['rectifier_terms'].append(rectifier_term)
                        stats['machine_mean_sum'] += machine_pred
                        # ğŸš¨ ì¶”ê°€: P_pos ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
                        stats['prob_pos_list'].append(prob_pos)

                except Exception:
                    continue

        print()

        # 3. ìµœì¢… í†µê³„ ê³„ì‚°
        final_golden_stats = {}
        for axis, stats in golden_stats.items():
            if stats['labeled_n'] > 0:
                final_golden_stats[axis] = {
                    'labeled_n': stats['labeled_n'],
                    'rectifier_terms': stats['rectifier_terms'],
                    'machine_mean': stats['machine_mean_sum'] / stats['labeled_n'],
                    # ğŸš¨ ì¶”ê°€: P_pos ë¦¬ìŠ¤íŠ¸ë¥¼ ìµœì¢… í†µê³„ì— í¬í•¨
                    'prob_pos_list': stats['prob_pos_list']
                }

        return final_golden_stats

    def calculate_ppi_asymptotic_ci(
            self,
            machine_preds: List[int],
            rectifiers: List[float],
            total_n: int,
            labeled_n: int
    ) -> float:
        """PPI Asymptotic CI Half-width ê³„ì‚°."""
        if labeled_n <= 1 or total_n <= 0:
            return 0.0

        y_hat_array: NDArray[np.float64] = np.array(machine_preds, dtype=np.float64)
        rectifier_array: NDArray[np.float64] = np.array(rectifiers, dtype=np.float64)

        sigma2_f: float = float(np.var(y_hat_array))
        sigma2_rec: float = float(np.var(rectifier_array))

        variance: float = (sigma2_f / float(total_n)) + (sigma2_rec / float(labeled_n))
        half_width: float = self.ci_z_score * math.sqrt(variance)
        return round(half_width, 3)

    def _calculate_binned_probs(self, probs_pos: List[float], total_n: int) -> List[Dict[str, float | int | str]]:
        """Softmax P_pos ê°’ì„ 10ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¹ˆë„ì™€ ë¹„ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        probs_array = np.array(probs_pos)
        # 0.0 to 1.0, 10 bins. range=(0.0, 1.000001)ì„ ì‚¬ìš©í•˜ì—¬ 1.0ì„ í¬í•¨í•©ë‹ˆë‹¤.
        counts, _ = np.histogram(probs_array, bins=10, range=(0.0, 1.000001))

        binned_data = []

        for i in range(10):
            low = i * 0.1
            high = (i + 1) * 0.1
            count = int(counts[i])
            percentage = round((count / total_n) * 100, 1) if total_n > 0 else 0.0

            binned_data.append({
                'range': f"{low:.1f} - {high:.1f}",
                'count': count,
                'percentage': percentage
            })

        # ì‹œê°í™”ë¥¼ ìœ„í•´ ìµœëŒ€ ë¹„ìœ¨ì„ ê³„ì‚° (report_utilì—ì„œ ë§‰ëŒ€ ê¸¸ì´ ì •ê·œí™”ìš©)
        max_perc = max(item['percentage'] for item in binned_data)

        # ëª¨ë“  ë¹ˆì— max_perc ê°’ì„ ì¶”ê°€
        for item in binned_data:
            item['max_perc'] = max_perc

        return binned_data

    def _calculate_confidence_stats(self, scores: List[int], probs_pos: List[float]) -> Dict[str, float]:
        """Softmax ê¸ì • í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ í™•ì‹ ë„ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (ì¤‘ì•™ê°’ í¬í•¨)"""

        scores_array = np.array(scores)
        probs_pos_array = np.array(probs_pos)
        probs_neg_array = 1.0 - probs_pos_array

        # 1. ê¸°ë³¸ ë¶„í¬ í†µê³„ ê³„ì‚° (P_pos ë° P_neg ê¸°ì¤€)
        stats = {
            'prob_pos_min': round(float(np.min(probs_pos_array)), 4),
            'prob_pos_avg': round(float(np.mean(probs_pos_array)), 4),
            'prob_pos_median': round(float(np.median(probs_pos_array)), 4),
            'prob_pos_max': round(float(np.max(probs_pos_array)), 4),

            'prob_neg_min': round(float(np.min(probs_neg_array)), 4),
            'prob_neg_avg': round(float(np.mean(probs_neg_array)), 4),
            'prob_neg_median': round(float(np.median(probs_neg_array)), 4),
            'prob_neg_max': round(float(np.max(probs_neg_array)), 4),
        }

        # 2. ì¡°ê±´ë¶€ í™•ì‹ ë„ ë° ë§ˆì§„ ê³„ì‚° (íŒŒìƒ ì§€í‘œ)

        margin_array = np.abs(probs_pos_array - probs_neg_array)
        stats['mean_margin'] = round(float(np.mean(margin_array)), 4)

        pos_mask = scores_array == 1
        num_pos_preds = np.sum(pos_mask)
        neg_mask = scores_array == 0
        num_neg_preds = np.sum(neg_mask)

        stats['conf_pos_avg'] = round(float(np.mean(probs_pos_array[pos_mask])), 4) if num_pos_preds > 0 else 0.0
        stats['conf_neg_avg'] = round(float(np.mean(probs_neg_array[neg_mask])), 4) if num_neg_preds > 0 else 0.0

        return stats


    def calculate_ppi_summary(
            self,
            file_base_name: str,
            current_lm_scores: Dict[str, List[int]],
            current_lm_probs: Dict[str, List[float]],
            total_n: int,
            golden_set_stats: Dict[str, Dict[str, Any]],
            golden_set_name: str
    ) -> Dict[str, Any]:
        """
        LM ì˜ˆì¸¡ ê²°ê³¼ì™€ ê³¨ë“ ì…‹ í†µê³„ë¥¼ ê²°í•©í•˜ì—¬ PPI ìš”ì•½ ê²°ê³¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        (í™•ì‹ ë„ í†µê³„, í‰ê°€ì…‹ Binning ë°ì´í„°, ê³¨ë“ ì…‹ Binning ë°ì´í„°ë¥¼ í¬í•¨)
        """

        summary: Dict[str, Any] = {
            "model_name": file_base_name,
            "golden_set_name": golden_set_name,
            "n": total_n,
            "ppi_active": True,
            "labeled_n_rep": golden_set_stats.get(KEY_CR, {}).get('labeled_n', 0),
        }

        overall_corrected_scores: List[float] = []

        for axis in JUDGE_TYPES:
            scores: List[int] = current_lm_scores[axis]
            probs_pos: List[float] = current_lm_probs[axis]

            golden_axis_stats = golden_set_stats.get(axis, {})
            labeled_n_axis: int = golden_axis_stats.get('labeled_n', 0)

            # í•„ìˆ˜ í†µê³„ ëˆ„ë½ ê²€ì‚¬ (ì´ì „ ë‹¨ê³„ ìˆ˜ì • ë¡œì§ ìœ ì§€)
            if labeled_n_axis == 0 or not golden_axis_stats:
                raise RuntimeError(
                    f"[{axis} ì¶• í†µê³„ ëˆ„ë½] ê³¨ë“ ì…‹ '{golden_set_name}'ì— '{axis}' ì¶•ì˜ ìœ íš¨í•œ ë¼ë²¨ë§ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. "
                    f"Labeled N: {labeled_n_axis}."
                )

            rectifier_terms: List[float] = golden_axis_stats['rectifier_terms']

            # 1. PPI í†µê³„ ê³„ì‚°
            machine_mean: float = sum(scores) / float(total_n)
            rectifier: float = sum(rectifier_terms) / labeled_n_axis
            corrected_mean: float = max(0.0, min(1.0, machine_mean - rectifier))
            margin: float = self.calculate_ppi_asymptotic_ci(scores, rectifier_terms, total_n, labeled_n_axis)

            # 2. í™•ì‹ ë„ í†µê³„ ê³„ì‚° (ê¸°ìˆ  í†µê³„ëŸ‰)
            confidence_stats = self._calculate_confidence_stats(scores, probs_pos)

            # 3. Binning ë°ì´í„° ê³„ì‚°
            # í‰ê°€ ëŒ€ìƒì…‹ Binning
            prob_bins = self._calculate_binned_probs(probs_pos, total_n)

            # ğŸš¨ ì¶”ê°€: ê³¨ë“ ì…‹ Binning ë°ì´í„° ê³„ì‚°
            golden_probs_pos: List[float] = golden_axis_stats['prob_pos_list']
            # labeled_n_axisëŠ” ê³¨ë“ ì…‹ì˜ ì´ ìƒ˜í”Œ ìˆ˜(N) ì—­í• ì„ í•©ë‹ˆë‹¤.
            golden_prob_bins = self._calculate_binned_probs(golden_probs_pos, labeled_n_axis)

            # 4. ìµœì¢… ìš”ì•½ì— ì¶”ê°€
            summary[axis] = {
                # PPI í†µê³„
                "machine_mean": round(machine_mean, 2),
                "corrected_mean": round(corrected_mean, 2),
                "applied_rectifier": round(rectifier, 3),
                "ci": round(margin, 2),
                # í™•ì‹ ë„ í†µê³„ í•©ë³‘
                **confidence_stats,
                # Softmax Binning ë°ì´í„°
                'prob_bins': prob_bins,  # í‰ê°€ì…‹ íˆìŠ¤í† ê·¸ë¨ìš©
                'golden_prob_bins': golden_prob_bins  # ğŸš¨ ì¶”ê°€: ê³¨ë“ ì…‹ íˆìŠ¤í† ê·¸ë¨ìš©
            }
            overall_corrected_scores.append(corrected_mean)

        if overall_corrected_scores:
            summary["overall"] = round(sum(overall_corrected_scores) / len(overall_corrected_scores), 2)

        return summary


# ===================================================================
# 4. ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜
# ===================================================================
# ì´ˆê¸°í™”
def _load_judges_and_calc() -> Tuple[AresJudge, PPICalculator]:
    """ì‹¬ì‚¬ê´€ê³¼ ê³„ì‚°ê¸° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë¡œë“œí•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    # í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±ì€ ë©”ì¸ íŒŒì´í”„ë¼ì¸ì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    try:
        judge = AresJudge()
        calculator = PPICalculator()
        return judge, calculator
    except RuntimeError as e:
        print(f"\n[FATAL ERROR] ARES ì‹¬ì‚¬ê´€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise


# ê³¨ë“ ì…‹ í‰ê°€
def _evaluate_golden_sets(judge: AresJudge, calculator: PPICalculator) -> Tuple[Dict, Dict]:
    """
    ê³¨ë“ ì…‹ ë””ë ‰í† ë¦¬ë¥¼ íƒìƒ‰í•˜ê³ , ê° ê³¨ë“ ì…‹ì„ í‰ê°€í•˜ì—¬ í†µê³„ ë§µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ê³¨ë“ ì…‹ í†µê³„ê°€ ì—†ìœ¼ë©´ Fatal Error ë°œìƒ)
    """
    GOLDEN_DIR = config.DATA_GOLDEN_DIR

    # 1. ê³¨ë“  ë¼ë²¨ íŒŒì¼ ê²€ìƒ‰
    golden_files: List[Tuple[str, str]] = []
    if os.path.isdir(GOLDEN_DIR):
        for filename in os.listdir(GOLDEN_DIR):
            if filename.endswith('.jsonl'):
                golden_name = filename.replace('.jsonl', '')
                file_path = os.path.join(GOLDEN_DIR, filename)
                golden_files.append((golden_name, file_path))

        print(f"\n[INFO] {GOLDEN_DIR}ì—ì„œ ì´ {len(golden_files)}ê°œì˜ ê³¨ë“ ì…‹ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    else:
        print(f"\n[WARN] ê³¨ë“ ì…‹ ë””ë ‰í† ë¦¬ '{GOLDEN_DIR}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    if not golden_files:
        raise RuntimeError("PPI ë³´ì •ì„ ìœ„í•œ ê³¨ë“  ë¼ë²¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # 2. ëª¨ë“  ê³¨ë“ ì…‹ í‰ê°€ ë° í†µê³„ ì €ì¥
    golden_stats_map: Dict[str, Dict[str, Dict[str, Any]]] = {}
    # ğŸš¨ ìˆ˜ì •: ìˆœìˆ˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ë¡œ ë³€ê²½ (Markdown ì•„ë‹˜)
    golden_report_data: Dict[str, Dict[str, Dict[str, float | str]]] = {}

    for golden_name, path in golden_files:
        try:
            stats = calculator.evaluate_golden_set(judge, path)
            if stats:
                golden_stats_map[golden_name] = stats

                # ğŸš¨ ìˆ˜ì •: ìˆœìˆ˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ ë° ì €ì¥
                report_data = calculator.generate_golden_set_stat(stats)
                golden_report_data[golden_name] = report_data

                print() # ì„±ê³µ ë¡œê·¸ ì•ì— ì¤„ë°”ê¿ˆì„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€í•˜ì—¬ ì´ì „ tqdm ì¤„ì„ ì •ë¦¬
                print(f"\n   [SUCCESS] ê³¨ë“ ì…‹ '{golden_name}' í‰ê°€ ì™„ë£Œ.")
            else:
                print(f"\n   [WARN] ê³¨ë“ ì…‹ '{golden_name}' í‰ê°€ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            print(f"\n[FATAL ERROR] ê³¨ë“ ì…‹ '{golden_name}' í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê±´ë„ˆëœë‹ˆë‹¤.")

    if not golden_stats_map:
        raise RuntimeError("PPI ë³´ì •ì„ ìœ„í•œ ìœ íš¨í•œ ê³¨ë“  ë¼ë²¨ ë°ì´í„° í‰ê°€ ì‹¤íŒ¨.")

    # ğŸš¨ ìˆ˜ì •: golden_report_data ë°˜í™˜
    return golden_stats_map, golden_report_data


# ëŒ€ê·œëª¨ í‰ê°€ ë£¨í”„
def _process_input_files(judge: AresJudge, calculator: PPICalculator, golden_stats_map: Dict) -> Tuple[
    List, int, float]:
    """
    ì…ë ¥ íŒŒì¼ì„ ìˆœíšŒí•˜ë©° LM ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , PPI ë³´ì •ì„ ì ìš©í•˜ì—¬ model_summariesë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    INPUT_DIR = config.DATA_IN_DIR
    input_files = [
        os.path.join(INPUT_DIR, f) for f in os.listdir(INPUT_DIR) if f.endswith('.jsonl')
    ]

    if not input_files:
        print(f"\n[WARN] QCA `.jsonl` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return [], 0, 0.0

    print(f"\n[INFO] í‰ê°€ ëŒ€ìƒ íŒŒì¼ ê°¯ìˆ˜ : {len(input_files)}")

    total_successful_evals = 0
    model_summaries: List[Dict[str, Any]] = []
    full_start_time = time.time()

    # 2-2. íŒŒì¼ë³„ í‰ê°€, LM ì˜ˆì¸¡ íŒŒì¼ ìƒì„± ë° ê²°ê³¼ ì§‘ê³„
    for file_path in input_files:
        start_time = time.time()
        file_base_name = os.path.basename(file_path).replace('.jsonl', '')

        # QCA í‰ê°€ (Judge)ëŠ” í•œ ë²ˆë§Œ ìˆ˜í–‰
        print(f"\n--- ëŒ€ê·œëª¨ í‰ê°€ ì‹œì‘: {file_base_name} ---")
        current_lm_scores = {k: [] for k in JUDGE_TYPES}
        current_lm_probs = {k: [] for k in JUDGE_TYPES}  # ğŸš¨ ìˆ˜ì •: Softmax ê¸ì • í™•ë¥  ì§‘ê³„ìš©
        # all_results_for_file = [] # ì €ì¥ ë¡œì§ ë¹„í™œì„±í™” ì‹œ ë¶ˆí•„ìš”
        processed_count_in_file = 0

        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            for line in tqdm(lines, desc=f"í‰ê°€ ì¤‘ [{file_base_name}]"):
                try:
                    data = json.loads(line.strip())
                    query = ' '.join(data.get('q', '').split()).strip()
                    context = ' '.join(data.get('c', '').split()).strip()
                    answer = ' '.join(data.get('a', '').split()).strip()

                    if not all([query, context, answer]): continue

                    scores_with_probs = judge.evaluate_triple(query, context, answer)

                    # all_results_for_file.append({**data, **scores_with_probs}) # ì €ì¥ ë¹„í™œì„±í™”

                    for axis in JUDGE_TYPES:
                        pred_key = JUDGE_PREDICTION_FIELDS[axis]
                        axis_scores = scores_with_probs.get(pred_key)

                        if axis_scores is None: continue

                        # ì˜ˆì¸¡ í´ë˜ìŠ¤ (Yhat) ì €ì¥ (PPI ë³´ì •ìš©)
                        current_lm_scores[axis].append(axis_scores.get('machine_pred', 0))

                        # ğŸš¨ ìˆ˜ì •: ê¸ì • í™•ë¥  (P_pos) ì €ì¥ (í™•ì‹ ë„ ê³„ì‚°ìš©)
                        current_lm_probs[axis].append(axis_scores.get('prob_pos', 0.0))

                    total_successful_evals += 1
                    processed_count_in_file += 1

                except Exception as e:
                    print(f"[ERROR] í‰ê°€ ì¤‘ ì˜ˆì™¸ë°œìƒ : {e}")
                    continue

        end_time = time.time()
        print(f"[INFO] í‰ê°€ ì†Œìš”ì‹œê°„ : {end_time - start_time:,.2f}ì´ˆ ")

        # 2-3. í‰ê°€ì…‹ë‹¹ ëª¨ë“  ê³¨ë“ ì…‹ì— ëŒ€í•´ PPI í†µê³„ ê³„ì‚° ë° ì§‘ê³„
        if processed_count_in_file > 0:
            for golden_name, golden_stats in golden_stats_map.items():
                summary = calculator.calculate_ppi_summary(
                    file_base_name,
                    current_lm_scores,
                    current_lm_probs,  # ğŸš¨ ìˆ˜ì •: í™•ë¥  ë°ì´í„° ì „ë‹¬
                    processed_count_in_file,
                    golden_stats,
                    golden_name
                )
                model_summaries.append(summary)

            print(f"   [ì§‘ê³„ ì™„ë£Œ] '{file_base_name}' ê²°ê³¼ ì§‘ê³„ ì™„ë£Œ. (ëª¨ë“  {len(golden_stats_map)}ê°œ ê³¨ë“ ì…‹ ì ìš©)")
        else:
            print(f"   [ERROR] ì‹¬ì‚¬ê´€ì˜ í‰ê°€ ê²°ê³¼ ì—†ìŒ - íŒŒì¼: {file_base_name}")

    full_elapsed_time = time.time() - full_start_time
    return model_summaries, total_successful_evals, full_elapsed_time


# ë³´ê³ ì„œ ë° ìš”ì•½
def _generate_report_and_summary(golden_markdown_map: Dict, model_summaries: List, total_successful_evals: int,
                                 full_elapsed_time: float) -> None:
    """
    ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ì½˜ì†”ì— ì‹¤í–‰ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    REPORT_DIR = config.DATA_REPORT_DIR
    MODEL_NAME = config.MODEL_NAME

    if model_summaries:
        report_content: str = report_util.generate_summary_report(golden_markdown_map, model_summaries)
        timestamp: str = time.strftime("%Y%m%d_%H%M%S")

        # íŒŒì¼ëª… ìˆ˜ì • ë°˜ì˜
        report_filename: str = f"{MODEL_NAME}_{timestamp}.md"
        output_path: str = os.path.join(REPORT_DIR, report_filename)

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        print(f"\n[ìµœì¢… ì™„ë£Œ] ARES í†µê³„ ë³´ê³ ì„œ ìƒì„±ë¨ â†’ {output_path}")

    print("\n\n=============== LM ì˜ˆì¸¡ ìƒì„± ìµœì¢… ìš”ì•½ ===============")
    print(f"ì´ LM ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: {total_successful_evals}ê°œ")
    print(f"ì´ ì†Œìš” ì‹œê°„: {full_elapsed_time:.2f}ì´ˆ")
    print("==================================================")



# ì‹¤í–‰
# 02_generate_ppi.py (run_ares_pipeline í•¨ìˆ˜)

def run_ares_pipeline():
    """
    ARES ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ê´€ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    """
    try:
        # 0. í™˜ê²½ ì„¤ì •
        os.makedirs(config.DATA_IN_DIR, exist_ok=True)
        os.makedirs(config.DATA_REPORT_DIR, exist_ok=True)
        print(f"\n[SETUP] í‰ê°€ëŒ€ìƒì¸ QCA ì…ë ¥ ë””ë ‰í† ë¦¬: {config.DATA_IN_DIR}, ë³´ê³ ì„œ ë””ë ‰í† ë¦¬: {config.DATA_REPORT_DIR}")

        # 1. ì´ˆê¸°í™” ë° ê³¨ë“ ì…‹ í‰ê°€
        judge, calculator = _load_judges_and_calc()
        # ğŸš¨ ìˆ˜ì •: golden_markdown_map ëŒ€ì‹  golden_report_data ë³€ìˆ˜ëª… ì‚¬ìš©
        golden_stats_map, golden_report_data = _evaluate_golden_sets(judge, calculator)

        # 2. ëŒ€ê·œëª¨ í‰ê°€ ì‹¤í–‰
        model_summaries, total_successful_evals, full_elapsed_time = _process_input_files(
            judge, calculator, golden_stats_map
        )

        # 3. ë³´ê³ ì„œ ìƒì„± ë° ìµœì¢… ìš”ì•½
        # ğŸš¨ ìˆ˜ì •: golden_report_data ì¸ìˆ˜ë¡œ ì „ë‹¬ (report_utilì—ì„œ MD ìƒì„±)
        _generate_report_and_summary(golden_report_data, model_summaries, total_successful_evals, full_elapsed_time)

    except RuntimeError as e:
        print(f"\n[FATAL ERROR] íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

if __name__ == "__main__":
    run_ares_pipeline()

