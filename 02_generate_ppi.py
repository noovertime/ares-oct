# 02_generate_ppi.py (í´ë˜ìŠ¤ ê¸°ë°˜ ë° ë‹¤ì¤‘ ê³¨ë“ ì…‹ í™•ì¥ ë²„ì „)
# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import json
import logging
import math
import os
import time
from typing import Dict, Any, List, Tuple

# 2. ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
from numpy.typing import NDArray
import torch
from scipy.stats import norm
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# 3. ë¡œì»¬/ì• í”Œë¦¬ì¼€ì´ì…˜ ê³ ìœ  ë¼ì´ë¸ŒëŸ¬ë¦¬
import config
import report_util
from config import (
    KEY_CR,
    KEY_AF,
    KEY_AR,
    JUDGE_TYPES,
    JUDGE_PREDICTION_FIELDS,
    GOLD_LABEL_FIELDS,
    TOKEN_TYPE_ID_OFF
)
from json_util import _load_json_lines  # ì™¸ë¶€ ëª¨ë“ˆ ê°€ì •

# ===================================================================
# 0. ì „ì—­ ìƒìˆ˜ ë° í™˜ê²½ ì„¤ì •
# ===================================================================

# Transformers ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™”
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)
logging.getLogger("transformers.modeling_utils").setLevel(logging.ERROR)

# ê²½ë¡œ ì„¤ì •
MODEL_DIR_BASE = config.MODEL_DIR
MODEL_NAME = config.MODEL_NAME

# CPU í™˜ê²½ ì„¤ì •
DEVICE = torch.device("cpu")
MAX_LENGTH = 128

# ARES ì‹¬ì‚¬ê´€ íƒ€ì… ë° í´ë” ë§¤í•‘
FOLDER_MAPPING = {
    config.KEY_CR: 'context_relevance',
    config.KEY_AF: 'answer_faithfulness',
    config.KEY_AR: 'answer_relevance'
}

CI_ALPHA: float = 0.05  # 95% ì‹ ë¢°êµ¬ê°„
CI_Z_SCORE: float = float(norm.ppf(1 - CI_ALPHA / 2))  # ì•½ 1.96


# ===================================================================
# 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (í´ë˜ìŠ¤ì— í¬í•¨ì‹œí‚¤ì§€ ì•ŠëŠ” ë²”ìš© ê¸°ëŠ¥)
# ===================================================================

def _find_model_path(judge_type: str) -> str:
    """ê³ ì •ëœ ì‹¬ì‚¬ê´€ ì´ë¦„ í´ë” ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # judge_typeì€ KEY_CR, KEY_AF, KEY_AR ì¤‘ í•˜ë‚˜
    target_folder = FOLDER_MAPPING.get(judge_type)

    if not target_folder:
        raise ValueError(f"ì •ì˜ë˜ì§€ ì•Šì€ ì‹¬ì‚¬ê´€ íƒ€ì…: {judge_type}")

    model_path = os.path.join(MODEL_DIR_BASE, target_folder)

    if not os.path.isdir(model_path):
        raise FileNotFoundError(f"ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    return model_path


def cleanup_evaluation_data():
    """
    configì— ì •ì˜ëœ PPI ì¶œë ¥ ë””ë ‰í† ë¦¬ì™€ ìµœì¢… ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    dirs_to_clean = [ config.DATA_REPORT_DIR]
    print("\n>> í‰ê°€ ë° ë³´ê³ ì„œ ì¶œë ¥ íŒŒì¼ ì •ë¦¬ ì‹œì‘...")
    for target_dir in dirs_to_clean:
        if os.path.isdir(target_dir):
            files_deleted = 0
            # ... (ê¸°ì¡´ cleanup_evaluation_data ë¡œì§ ìœ ì§€)
            for filename in os.listdir(target_dir):
                file_path = os.path.join(target_dir, filename)
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        files_deleted += 1
                except Exception as e:
                    print(f"   [ERROR] íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")

            print(f"   [SUCCESS] '{target_dir}' ë””ë ‰í† ë¦¬ì—ì„œ ì´ {files_deleted}ê°œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ.")
        else:
            print(f"   [INFO] ë””ë ‰í† ë¦¬ '{target_dir}'ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì •ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")


# ===================================================================
# 2. ARES í‰ê°€ í´ë˜ìŠ¤
# ===================================================================

class AresJudge:
    """ARES ì‹¬ì‚¬ê´€(í† í¬ë‚˜ì´ì € ë° 3ê°œ ëª¨ë¸) ë¡œë”© ë° í‰ê°€ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤."""

    def __init__(self, device: torch.device = DEVICE, max_length: int = MAX_LENGTH):
        self.tokenizer: AutoTokenizer = None
        self.judges: Dict[str, AutoModelForSequenceClassification] = {}
        self.device = device
        self.max_length = max_length
        self._load_models()

    def _load_models(self):
        """CR, AF, AR ì„¸ ê°€ì§€ ì‹¬ì‚¬ê´€ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        print("\n>> ARES ì‹¬ì‚¬ê´€ ë¡œë”© ì‹œì‘ (CPU í™˜ê²½)...")

        # 1. í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        try:
            cr_path = _find_model_path(KEY_CR)
            self.tokenizer = AutoTokenizer.from_pretrained(cr_path, trust_remote_code=True)
            print(f"   [INFO] {cr_path} ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ.")
        except Exception as e:
            print(f"   [WARN] ì €ì¥ ê²½ë¡œì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨. ì›ë³¸ ëª¨ë¸ ({MODEL_NAME}) ë¡œë“œ ì‹œë„.")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
            except Exception as fallback_e:
                print(f"   [FATAL] í† í¬ë‚˜ì´ì € ë¡œë“œ ìµœì¢… ì‹¤íŒ¨: {fallback_e}")
                raise fallback_e

        if TOKEN_TYPE_ID_OFF:
            self.tokenizer.model_input_names = [
                name for name in self.tokenizer.model_input_names if name != 'token_type_ids'
            ]
            print("   [INFO] DistilBERT í˜¸í™˜ì„±ì„ ìœ„í•´ í† í¬ë‚˜ì´ì €ì˜ 'token_type_ids' ìƒì„±ì„ ë¹„í™œì„±í™”í–ˆìŠµë‹ˆë‹¤.")

        # 2. ëª¨ë¸ ë¡œë“œ (AutoModelForSequenceClassification ì‚¬ìš©)
        for judge_type in JUDGE_TYPES:
            try:
                model_path = _find_model_path(judge_type)
                model = AutoModelForSequenceClassification.from_pretrained(
                    model_path, num_labels=2, trust_remote_code=True
                )
                model.to(self.device)
                model.eval()
                self.judges[judge_type] = model
                print(f"   [SUCCESS] {judge_type} Judge ë¡œë“œ ì™„ë£Œ ( {model_path} )")

            except Exception as e:
                print(f"   [ERROR] {judge_type} Judge ë¡œë“œ ì‹¤íŒ¨: {e}. ì´ ëª¨ë¸ì€ ê±´ë„ˆëœë‹ˆë‹¤.")

        if len(self.judges) != 3:
            raise RuntimeError(f"ì´ {len(self.judges)}ê°œë§Œ ë¡œë“œë¨. ARES í‰ê°€ë¥¼ ìœ„í•´ 3ê°œ ëª¨ë¸ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤.")

        print(f">> ARES ì‹¬ì‚¬ê´€ {MODEL_NAME} ë¡œë“œ ì™„ë£Œ. ì´ {len(self.judges)}ê°œ ì‹¬ì‚¬ê´€ í™œì„±í™”.")

    def evaluate_triple(self, query: str, context: str, answer: str) -> Dict[str, int]:
        """í•˜ë‚˜ì˜ Q-C-A ìŒì— ëŒ€í•´ 3ê°€ì§€ ARES ì ìˆ˜ (0 ë˜ëŠ” 1)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""

        results = {}

        judge_inputs = {
            JUDGE_PREDICTION_FIELDS[KEY_CR]: (query, context, self.judges[KEY_CR]),
            JUDGE_PREDICTION_FIELDS[KEY_AF]: (context, answer, self.judges[KEY_AF]),
            JUDGE_PREDICTION_FIELDS[KEY_AR]: (query, answer, self.judges[KEY_AR])
        }

        with torch.no_grad():
            for name, (text_a, text_b, model) in judge_inputs.items():
                # 1. ì…ë ¥ í† í°í™”
                inputs = self.tokenizer(
                    text_a, text_b,
                    return_tensors="pt",
                    truncation=True,
                    padding='max_length',
                    max_length=self.max_length
                ).to(self.device)

                # 2. ì˜ˆì¸¡ ìˆ˜í–‰ ë° ê²°ê³¼ ì‚°ì¶œ
                outputs = model(**inputs)
                prediction = torch.argmax(outputs.logits, dim=1).item()
                results[name] = prediction

        return results


# ===================================================================
# 3. PPI í†µê³„ ë° ë³´ì • ê³„ì‚° í´ë˜ìŠ¤
# ===================================================================

class PPICalculator:
    """PPI ë³´ì • ë° í†µê³„ ê³„ì‚°ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤."""

    def __init__(self, ci_z_score: float = CI_Z_SCORE):
        self.ci_z_score = ci_z_score

    def calculate_accuracy(self, stats: Dict[str, Any]) -> str:
        """rectifier_termsë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¬ì‚¬ê´€ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³  ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        terms = stats.get('rectifier_terms')
        labeled_n = stats.get('labeled_n')

        if not terms or labeled_n == 0:
            return 'N/A'

        # ì˜¤ë¥˜ íšŸìˆ˜ = (rectifier_terms ë¦¬ìŠ¤íŠ¸ì—ì„œ 0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜)
        error_count = sum(1 for term in terms if term != 0.0)
        accuracy = 1.0 - (error_count / labeled_n)
        return f"{accuracy:13.3f}"

    def generate_golden_set_stat(self, golden_set_stats: Dict[str, Dict[str, Any]]) -> str:
        """ê³¨ë“ ì…‹ í†µê³„ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not golden_set_stats:
            print("[FAIL] ê³¨ë“ ì…‹ í‰ê°€ ê²°ê³¼ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return "ê³¨ë“ ì…‹ í‰ê°€ ê²°ê³¼ ì—†ìŒ"

        # ... (ê¸°ì¡´ generate_golden_set_stat ë¡œì§ ìœ ì§€)
        cr_mean = golden_set_stats.get(KEY_CR, {}).get('machine_mean', 'N/A')
        af_mean = golden_set_stats.get(KEY_AF, {}).get('machine_mean', 'N/A')
        ar_mean = golden_set_stats.get(KEY_AR, {}).get('machine_mean', 'N/A')

        cr_acc = self.calculate_accuracy(golden_set_stats.get(KEY_CR, {}))
        af_acc = self.calculate_accuracy(golden_set_stats.get(KEY_AF, {}))
        ar_acc = self.calculate_accuracy(golden_set_stats.get(KEY_AR, {}))

        header = f"| {'êµ¬ë¶„':^12} | {'CR':^13} | {'AF':^13} | {'AR':^13} |"
        separator = f"+----------------+---------------+---------------+---------------+"
        mean_row = (
            f"| {'ì˜ˆì¸¡í‰ê· ':^12} | {cr_mean:>13.3f} | {af_mean:>13.3f} | {ar_mean:>13.3f} |"
            f"  ì‹¬ì‚¬ê´€ì´ 1ì´ë¼ê³  ì˜ˆì¸¡í•œ ë¹„ìœ¨ (ê¸ì • í¸í–¥)"
        )
        acc_row = (
            f"| {'ì •ë‹µë¹„ìœ¨':^12} | {cr_acc:>13} | {af_acc:>13} | {ar_acc:>13} |"
            f"  ì‹¬ì‚¬ê´€ ì˜ˆì¸¡ì˜ ì •í™•ë„"
        )
        markdown_content = "\n".join([header, separator, mean_row, acc_row])
        markdown_string = f"```\n{markdown_content}\n```"
        return markdown_string

    def evaluate_golden_set(self, judge: AresJudge, golden_set_filepath: str) -> Dict[str, Dict[str, Any]]:
        """
        íŠ¹ì • ê³¨ë“ ì…‹ íŒŒì¼ì„ ì‹¬ì‚¬ê´€ì´ í‰ê°€í•˜ê³ , PPI í¸í–¥ ê³„ì‚°ì— í•„ìš”í•œ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        (ê¸°ì¡´ì˜ load_gold_labels_map ë¡œì§ì€ ì´ í•¨ìˆ˜ì— í†µí•©ë˜ì–´ ID ê¸°ë°˜ ë§¤ì¹­ ì—†ì´ ì§ì ‘ í‰ê°€)
        """
        golden_stats = {key: {'labeled_n': 0, 'rectifier_terms': [], 'machine_mean_sum': 0.0} for key in JUDGE_TYPES}
        golden_records = _load_json_lines(golden_set_filepath)

        if not golden_records:
            print(f"[WARN] ê³¨ë“ ì…‹ íŒŒì¼ {golden_set_filepath}ì— í‰ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        print(f"\n>> ê³¨ë“ ì…‹ í‰ê°€ ì‹œì‘. {golden_set_filepath}, ì´ {len(golden_records)}ê°œ ìƒ˜í”Œ ì‹¬ì‚¬ê´€ ì˜ˆì¸¡ ì¤‘...")

        for data in tqdm(golden_records, desc="ê³¨ë“ ì…‹ ì‹¬ì‚¬ê´€ í‰ê°€ ì¤‘"):
            try:
                # Q, C, A ì¶”ì¶œ ë° ì •ê·œí™”
                query = ' '.join(data.get('q', '').split()).strip()
                context = ' '.join(data.get('c', '').split()).strip()
                answer = ' '.join(data.get('a', '').split()).strip()

                if not all([query, context, answer]):
                    continue

                # 1. LM ì‹¬ì‚¬ê´€ ì˜ˆì¸¡ (Yhat_labeled)
                scores = judge.evaluate_triple(query, context, answer)

                # 2. LM ì˜ˆì¸¡ê°’ê³¼ ì¸ê°„ ì£¼ì„ê°’ ë¹„êµ (Y_labeledëŠ” ê³¨ë“ ì…‹ íŒŒì¼ì—ì„œ ì§ì ‘ ì¶”ì¶œ)
                for axis in JUDGE_TYPES:
                    machine_pred = scores.get(JUDGE_PREDICTION_FIELDS[axis])
                    gold_key = GOLD_LABEL_FIELDS[axis]
                    gold_label = data.get(gold_key)

                    if machine_pred is None or gold_label is None:
                        continue

                    # í†µê³„ ì—…ë°ì´íŠ¸
                    machine_pred = int(machine_pred)
                    gold_label = int(gold_label)

                    rectifier_term = float(machine_pred - gold_label)

                    stats = golden_stats[axis]
                    stats['labeled_n'] += 1
                    stats['rectifier_terms'].append(rectifier_term)
                    stats['machine_mean_sum'] += machine_pred

            except Exception:
                continue

        # 3. ìµœì¢… í†µê³„ ê³„ì‚°
        final_golden_stats = {}
        for axis, stats in golden_stats.items():
            if stats['labeled_n'] > 0:
                final_golden_stats[axis] = {
                    'labeled_n': stats['labeled_n'],
                    'rectifier_terms': stats['rectifier_terms'],
                    'machine_mean': stats['machine_mean_sum'] / stats['labeled_n']
                }

        return final_golden_stats

    def calculate_ppi_asymptotic_ci(
            self,
            machine_preds: List[int],
            rectifiers: List[float],
            total_n: int,
            labeled_n: int
    ) -> float:
        """PPI Asymptotic CI Half-width ê³„ì‚°."""
        if labeled_n <= 1 or total_n <= 0:
            return 0.0

        y_hat_array: NDArray[np.float64] = np.array(machine_preds, dtype=np.float64)
        rectifier_array: NDArray[np.float64] = np.array(rectifiers, dtype=np.float64)

        sigma2_f: float = float(np.var(y_hat_array))
        sigma2_rec: float = float(np.var(rectifier_array))

        variance: float = (sigma2_f / float(total_n)) + (sigma2_rec / float(labeled_n))
        half_width: float = self.ci_z_score * math.sqrt(variance)
        return round(half_width, 3)

    # PPICalculator í´ë˜ìŠ¤ ë‚´ë¶€ì˜ calculate_ppi_summary ë©”ì„œë“œ

    def calculate_ppi_summary(
            self,
            file_base_name: str,
            current_lm_scores: Dict[str, List[int]],
            total_n: int,
            golden_set_stats: Dict[str, Dict[str, Any]],
            golden_set_name: str
    ) -> Dict[str, Any]:
        """
        LM ì˜ˆì¸¡ ê²°ê³¼ì™€ ê³¨ë“ ì…‹ í†µê³„ë¥¼ ê²°í•©í•˜ì—¬ PPI ìš”ì•½ ê²°ê³¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        (ê³¨ë“ ì…‹ í†µê³„ê°€ ëˆ„ë½ë˜ë©´ ëª…ì‹œì  ì˜¤ë¥˜ ë°œìƒ)
        """

        summary: Dict[str, Any] = {
            "model_name": file_base_name,
            "golden_set_name": golden_set_name,
            "n": total_n,
            "ppi_active": True,
            # CR ê¸°ì¤€ìœ¼ë¡œ ëŒ€í‘œê°’ ì‚¬ìš©. í†µê³„ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ê¸°ë³¸ê°’ 0 ì„¤ì •
            "labeled_n_rep": golden_set_stats.get(KEY_CR, {}).get('labeled_n', 0),
        }

        overall_corrected_scores: List[float] = []

        for axis in JUDGE_TYPES:
            # 1. LM ì˜ˆì¸¡ ê²°ê³¼ (Yhat_unlabeled)
            scores: List[int] = current_lm_scores[axis]

            # 2. ê³¨ë“ ì…‹ í†µê³„ (Rectifier Terms, labeled_n)
            golden_axis_stats = golden_set_stats.get(axis, {})

            labeled_n_axis: int = golden_axis_stats.get('labeled_n', 0)

            # ğŸš¨ í•µì‹¬ ìˆ˜ì •: í•„ìˆ˜ í†µê³„ ëˆ„ë½ ì‹œ ëª…ì‹œì  ì˜¤ë¥˜ ë°œìƒ
            if labeled_n_axis == 0 or not golden_axis_stats:
                raise RuntimeError(
                    f"[{axis} ì¶• í†µê³„ ëˆ„ë½] ê³¨ë“ ì…‹ '{golden_set_name}'ì— '{axis}' ì¶•ì˜ ìœ íš¨í•œ ë¼ë²¨ë§ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. "
                    f"Labeled N: {labeled_n_axis}."
                )

            rectifier_terms: List[float] = golden_axis_stats['rectifier_terms']

            # 3. í†µê³„ ê³„ì‚°
            machine_mean: float = sum(scores) / float(total_n)

            # 3-2. í¸í–¥ (Rectifier) = Avg(Yhat_labeled - Y_labeled)
            # labeled_n_axisëŠ” 0ì´ ì•„ë‹˜ì„ ìœ„ì—ì„œ ë³´ì¥í–ˆìŒ
            rectifier: float = sum(rectifier_terms) / labeled_n_axis

            # 3-3. ë³´ì •ëœ ì„±ëŠ¥ (Corrected Mean)
            corrected_mean: float = max(0.0, min(1.0, machine_mean - rectifier))

            # 3-4. ì‹ ë¢° êµ¬ê°„ (CI)
            margin: float = self.calculate_ppi_asymptotic_ci(scores, rectifier_terms, total_n, labeled_n_axis)

            # 4. ìµœì¢… ìš”ì•½ì— ì¶”ê°€
            summary[axis] = {
                "machine_mean": round(machine_mean, 2),
                "corrected_mean": round(corrected_mean, 2),
                "applied_rectifier": round(rectifier, 3),
                "ci": round(margin, 2)
            }
            overall_corrected_scores.append(corrected_mean)

        if overall_corrected_scores:
            summary["overall"] = round(sum(overall_corrected_scores) / len(overall_corrected_scores), 2)

        return summary


# ===================================================================
# 4. ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜
# ===================================================================

def run_ares_pipeline():
    """
    ARES ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: ë‹¤ì¤‘ ê³¨ë“ ì…‹ ì²˜ë¦¬ ë° LM ì˜ˆì¸¡ íŒŒì¼ ìƒì„±/ë³´ê³ ì„œ ìƒì„±.
    """

    # --- 1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™” ë‹¨ê³„ ---
    INPUT_DIR = config.DATA_IN_DIR
    REPORT_DIR = config.DATA_REPORT_DIR
    GOLDEN_DIR = config.DATA_GOLDEN_DIR

    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(REPORT_DIR, exist_ok=True)
    print(f"\n[SETUP] í‰ê°€ëŒ€ìƒì¸ QCA ì…ë ¥ ë””ë ‰í† ë¦¬: {INPUT_DIR}")

    # í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    try:
        judge = AresJudge()
        calculator = PPICalculator()
    except RuntimeError as e:
        print(f"\n[FATAL ERROR] ARES ì‹¬ì‚¬ê´€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return

    # 1-2. ê³¨ë“  ë¼ë²¨ íŒŒì¼ ê²€ìƒ‰ (ë‹¤ì¤‘ ê³¨ë“ ì…‹ì„ ì²˜ë¦¬í•˜ë„ë¡ í™•ì¥)
    # ê²°ê³¼: golden_files = [ ('golden_set_name_1', '/path/to/file1.jsonl'), ... ]
    golden_files: List[Tuple[str, str]] = []
    if os.path.isdir(GOLDEN_DIR):
        for filename in os.listdir(GOLDEN_DIR):
            if filename.endswith('.jsonl'):
                # íŒŒì¼ëª…ì„ ê³¨ë“ ì…‹ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤ (í™•ì¥ì ì œê±°)
                golden_name = filename.replace('.jsonl', '')
                file_path = os.path.join(GOLDEN_DIR, filename)
                golden_files.append((golden_name, file_path))

        print(f"\n[INFO] {GOLDEN_DIR}ì—ì„œ ì´ {len(golden_files)}ê°œì˜ ê³¨ë“ ì…‹ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    else:
        print(f"\n[WARN] ê³¨ë“ ì…‹ ë””ë ‰í† ë¦¬ '{GOLDEN_DIR}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    if not golden_files:
        print("\n[FATAL ERROR] PPI ë³´ì •ì„ ìœ„í•œ ê³¨ë“  ë¼ë²¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # 1-3. ëª¨ë“  ê³¨ë“ ì…‹ í‰ê°€ ë° í†µê³„ ì €ì¥ (ë©”ëª¨ë¦¬ ì²˜ë¦¬)
    # golden_stats_map: { 'golden_set_name': { 'KEY_CR': { stats } }, ... }
    golden_stats_map: Dict[str, Dict[str, Dict[str, Any]]] = {}
    golden_markdown_map: Dict[str, str] = {}

    for golden_name, path in golden_files:
        try:
            stats = calculator.evaluate_golden_set(judge, path)
            if stats:
                golden_stats_map[golden_name] = stats
                golden_markdown_map[golden_name] = calculator.generate_golden_set_stat(stats)
                print(f"\n   [SUCCESS] ê³¨ë“ ì…‹ '{golden_name}' í‰ê°€ ì™„ë£Œ.")
            else:
                print(f"\n   [WARN] ê³¨ë“ ì…‹ '{golden_name}' í‰ê°€ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            print(f"\n[FATAL ERROR] ê³¨ë“ ì…‹ '{golden_name}' í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê±´ë„ˆëœë‹ˆë‹¤.")

    if not golden_stats_map:
        print("\n[FATAL ERROR] PPI ë³´ì •ì„ ìœ„í•œ ìœ íš¨í•œ ê³¨ë“  ë¼ë²¨ ë°ì´í„° í‰ê°€ ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # --- 2. ì…ë ¥ íŒŒì¼ ê²€ìƒ‰ ë° LM ì˜ˆì¸¡ ìƒì„± ë£¨í”„ ë‹¨ê³„ (ë‹¤ì¤‘ ê³¨ë“ ì…‹ìœ¼ë¡œ í™•ì¥ë¨) ---
    input_files = [
        os.path.join(INPUT_DIR, f) for f in os.listdir(INPUT_DIR) if f.endswith('.jsonl')
    ]

    if not input_files:
        print(f"\n[WARN] QCA `.jsonl` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    print(f"\n[INFO] í‰ê°€ ëŒ€ìƒ íŒŒì¼ ê°¯ìˆ˜ : {len(input_files)}")

    total_successful_evals = 0
    full_start_time = time.time()

    # ìµœì¢… ë³´ê³ ì„œì— ë“¤ì–´ê°ˆ ìš”ì•½ ë¦¬ìŠ¤íŠ¸ (ë‹¤ì¤‘ ê³¨ë“ ì…‹ * ë‹¤ì¤‘ í‰ê°€ì…‹ ì¡°í•©)
    model_summaries: List[Dict[str, Any]] = []

    # 2-2. íŒŒì¼ë³„ í‰ê°€, LM ì˜ˆì¸¡ íŒŒì¼ ìƒì„± ë° ê²°ê³¼ ì§‘ê³„
    for file_path in input_files:
        start_time = time.time()  # íŒŒì¼ë³„ ì‹œê°„ ì¸¡ì • ì‹œì‘
        file_base_name = os.path.basename(file_path).replace('.jsonl', '')

        # QCA í‰ê°€ (Judge)ëŠ” í•œ ë²ˆë§Œ ìˆ˜í–‰
        print(f"\n--- ëŒ€ê·œëª¨ í‰ê°€ ì‹œì‘: {file_base_name} ---")
        current_lm_scores = {k: [] for k in JUDGE_TYPES}
        all_results_for_file = []  # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSONL íŒŒì¼ë¡œ ì¶œë ¥í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ìœ ì§€
        processed_count_in_file = 0

        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            for line in tqdm(lines, desc=f"í‰ê°€ ì¤‘ [{file_base_name}]"):
                try:
                    data = json.loads(line.strip())
                    query = ' '.join(data.get('q', '').split()).strip()
                    context = ' '.join(data.get('c', '').split()).strip()
                    answer = ' '.join(data.get('a', '').split()).strip()

                    if not all([query, context, answer]): continue

                    # ARES ì‹¬ì‚¬ê´€ ì˜ˆì¸¡ ìˆ˜í–‰ (Yhat_unlabeled ìƒì„±)
                    scores = judge.evaluate_triple(query, context, answer)
                    data.update(scores)

                    # LM ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í˜„ì¬ íŒŒì¼ì˜ ì§‘ê³„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    for axis in JUDGE_TYPES:
                        pred_key = JUDGE_PREDICTION_FIELDS[axis]
                        current_lm_scores[axis].append(scores.get(pred_key, 0))

                    all_results_for_file.append(data)
                    total_successful_evals += 1
                    processed_count_in_file += 1

                except Exception as e:
                    print(f"[ERROR] í‰ê°€ ì¤‘ ì˜ˆì™¸ë°œìƒ : {e}")
                    continue

        end_time = time.time()
        print(f"[INFO] í‰ê°€ ì†Œìš”ì‹œê°„ : {end_time - start_time:,.2f}ì´ˆ ")

        # 2-3. í‰ê°€ì…‹ë‹¹ ëª¨ë“  ê³¨ë“ ì…‹ì— ëŒ€í•´ PPI í†µê³„ ê³„ì‚° ë° ì§‘ê³„
        if processed_count_in_file > 0:
            for golden_name, golden_stats in golden_stats_map.items():
                summary = calculator.calculate_ppi_summary(
                    file_base_name,
                    current_lm_scores,
                    processed_count_in_file,
                    golden_stats,
                    golden_name  # ê³¨ë“ ì…‹ ì´ë¦„ ì „ë‹¬
                )
                model_summaries.append(summary)

            print(f"   [ì§‘ê³„ ì™„ë£Œ] '{file_base_name}' ê²°ê³¼ ì§‘ê³„ ì™„ë£Œ. (ëª¨ë“  {len(golden_stats_map)}ê°œ ê³¨ë“ ì…‹ ì ìš©)")

            # --- LM ì˜ˆì¸¡ íŒŒì¼ (Yhat_unlabeled) ì €ì¥ ë¡œì§ (í•„ìš”í•˜ë‹¤ë©´ í™œì„±í™”) ---
            # output_filename = f"{file_base_name}_lm_preds.jsonl"
            # output_path = os.path.join(OUTPUT_DIR, output_filename)
            # with open(output_path, 'w', encoding='utf-8') as outfile:
            #     for result in all_results_for_file:
            #         outfile.write(json.dumps(result, ensure_ascii=False) + '\n')
            # print(f"   [SUCCESS] LM ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {output_path}")

        else:
            print(f"   [ERROR] ì‹¬ì‚¬ê´€ì˜ í‰ê°€ ê²°ê³¼ ì—†ìŒ - íŒŒì¼: {file_base_name}")

    # --- 3. ìµœì¢… ë³´ê³ ì„œ ìƒì„± ë‹¨ê³„ ---
    full_end_time = time.time()
    full_elapsed_time = full_end_time - full_start_time

    # --- 3. ìµœì¢… ë³´ê³ ì„œ ìƒì„± ë‹¨ê³„ ---
    # ğŸš¨ ë””ë²„ê¹…ì„ ìœ„í•´ ì´ ë¶€ë¶„ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.
    print(f"\n[DEBUG] Model Summaries ì²« ë²ˆì§¸ ìš”ì†Œ: {model_summaries[0]}")

    # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
    if model_summaries:
        # report_util.generate_summary_report í•¨ìˆ˜ëŠ” ì´ì œ golden_markdown_mapì„ ë°›ë„ë¡ ë³€ê²½ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        report_content: str = report_util.generate_summary_report(golden_markdown_map, model_summaries)
        timestamp: str = time.strftime("%Y%m%d_%H%M%S")
        report_filename: str = f"{MODEL_NAME}_{timestamp}.md"
        output_path: str = os.path.join(REPORT_DIR, report_filename)

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        print(f"\n[ìµœì¢… ì™„ë£Œ] ARES í†µê³„ ë³´ê³ ì„œ ìƒì„±ë¨ â†’ {output_path}")

    print("\n\n=============== LM ì˜ˆì¸¡ ìƒì„± ìµœì¢… ìš”ì•½ ===============")
    print(f"ì´ LM ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: {total_successful_evals}ê°œ")
    print(f"ì´ ì†Œìš” ì‹œê°„: {full_elapsed_time:.2f}ì´ˆ")
    print("==================================================")


if __name__ == "__main__":
    run_ares_pipeline()