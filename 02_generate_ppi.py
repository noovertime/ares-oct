# 02_generate_ppi.py (í´ë˜ìŠ¤ ê¸°ë°˜ ë° ë‹¤ì¤‘ ê³¨ë“ ì…‹ í™•ì¥ ë²„ì „)
# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import json
import logging
import math
import os
import time
from typing import Dict, Any, List, Tuple

# 2. ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
from numpy.typing import NDArray
import torch
from scipy.stats import norm
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# 3. ë¡œì»¬/ì• í”Œë¦¬ì¼€ì´ì…˜ ê³ ìœ  ë¼ì´ë¸ŒëŸ¬ë¦¬
import config
import report_util
from config import (
    KEY_CR,
    KEY_AF,
    KEY_AR,
    JUDGE_TYPES,
    JUDGE_PREDICTION_FIELDS,
    GOLD_LABEL_FIELDS,
    TOKEN_TYPE_ID_OFF
)
from json_util import _load_json_lines  # ì™¸ë¶€ ëª¨ë“ˆ ê°€ì •

# ===================================================================
# 0. ì „ì—­ ìƒìˆ˜ ë° í™˜ê²½ ì„¤ì •
# ===================================================================

# Transformers ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™”
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)
logging.getLogger("transformers.modeling_utils").setLevel(logging.ERROR)

# ê²½ë¡œ ì„¤ì •
MODEL_DIR_BASE = config.MODEL_DIR
MODEL_NAME = config.MODEL_NAME

# CPU í™˜ê²½ ì„¤ì •
DEVICE = torch.device("cpu")
MAX_LENGTH = 128

# ARES ì‹¬ì‚¬ê´€ íƒ€ì… ë° í´ë” ë§¤í•‘
FOLDER_MAPPING = {
    config.KEY_CR: 'context_relevance',
    config.KEY_AF: 'answer_faithfulness',
    config.KEY_AR: 'answer_relevance'
}

CI_ALPHA: float = 0.05  # 95% ì‹ ë¢°êµ¬ê°„
CI_Z_SCORE: float = float(norm.ppf(1 - CI_ALPHA / 2))  # ì•½ 1.96


# ===================================================================
# 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (í´ë˜ìŠ¤ì— í¬í•¨ì‹œí‚¤ì§€ ì•ŠëŠ” ë²”ìš© ê¸°ëŠ¥)
# ===================================================================

def _find_model_path(judge_type: str) -> str:
    """ê³ ì •ëœ ì‹¬ì‚¬ê´€ ì´ë¦„ í´ë” ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # judge_typeì€ KEY_CR, KEY_AF, KEY_AR ì¤‘ í•˜ë‚˜
    target_folder = FOLDER_MAPPING.get(judge_type)

    if not target_folder:
        raise ValueError(f"ì •ì˜ë˜ì§€ ì•Šì€ ì‹¬ì‚¬ê´€ íƒ€ì…: {judge_type}")

    model_path = os.path.join(MODEL_DIR_BASE, target_folder)

    if not os.path.isdir(model_path):
        raise FileNotFoundError(f"ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    return model_path


def cleanup_evaluation_data():
    """
    configì— ì •ì˜ëœ PPI ì¶œë ¥ ë””ë ‰í† ë¦¬ì™€ ìµœì¢… ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    dirs_to_clean = [ config.DATA_REPORT_DIR]
    print("\n>> í‰ê°€ ë° ë³´ê³ ì„œ ì¶œë ¥ íŒŒì¼ ì •ë¦¬ ì‹œì‘...")
    for target_dir in dirs_to_clean:
        if os.path.isdir(target_dir):
            files_deleted = 0
            # ... (ê¸°ì¡´ cleanup_evaluation_data ë¡œì§ ìœ ì§€)
            for filename in os.listdir(target_dir):
                file_path = os.path.join(target_dir, filename)
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                        files_deleted += 1
                except Exception as e:
                    print(f"   [ERROR] íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")

            print(f"   [SUCCESS] '{target_dir}' ë””ë ‰í† ë¦¬ì—ì„œ ì´ {files_deleted}ê°œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ.")
        else:
            print(f"   [INFO] ë””ë ‰í† ë¦¬ '{target_dir}'ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì •ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")


# ===================================================================
# 2. ARES í‰ê°€ í´ë˜ìŠ¤
# ===================================================================
class AresJudge:
    """ARES ì‹¬ì‚¬ê´€(í† í¬ë‚˜ì´ì € ë° 3ê°œ ëª¨ë¸) ë¡œë”© ë° í‰ê°€ ë‹´ë‹¹ í´ë˜ìŠ¤."""

    def __init__(self, device: torch.device = DEVICE, max_length: int = MAX_LENGTH) -> None:
        self.tokenizer: AutoTokenizer | None = None
        self.judges: Dict[str, AutoModelForSequenceClassification] = {}
        self.device = device
        self.max_length = max_length
        self._load_models()

    def _load_models(self) -> None:
        """CR, AF, AR ì„¸ ê°€ì§€ ì‹¬ì‚¬ê´€ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ."""
        print("\n>> ARES ì‹¬ì‚¬ê´€ ë¡œë”© ì‹œì‘...")

        # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
        cr_path = _find_model_path(KEY_CR)
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(cr_path, trust_remote_code=True)
            print(f"[INFO] í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ: {cr_path}")
        except Exception:
            print(f"[WARN] {cr_path} ì—ì„œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸({MODEL_NAME}) ë¡œë“œ ì‹œë„")
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
            print(f"[INFO] ê¸°ë³¸ ëª¨ë¸ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")

        # DistilBERT í˜¸í™˜: token_type_ids ì œê±°
        if TOKEN_TYPE_ID_OFF and self.tokenizer:
            self.tokenizer.model_input_names = [
                n for n in self.tokenizer.model_input_names if n != "token_type_ids"
            ]
            print("[INFO] 'token_type_ids' ë¹„í™œì„±í™” ì™„ë£Œ")

        # 2. ëª¨ë¸ ë¡œë“œ
        for judge_type in JUDGE_TYPES:
            try:
                path = _find_model_path(judge_type)
                model = AutoModelForSequenceClassification.from_pretrained(
                    path, num_labels=2, trust_remote_code=True
                ).to(self.device)
                model.eval()
                self.judges[judge_type] = model
                print(f"[SUCCESS] {judge_type} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({path})")
            except Exception as e:
                print(f"[ERROR] {judge_type} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

        if len(self.judges) != 3:
            raise RuntimeError(
                f"í•„ìš”í•œ ëª¨ë¸ 3ê°œ ì¤‘ {len(self.judges)}ê°œë§Œ ë¡œë“œë¨ â€” ëª¨ë“  ì‹¬ì‚¬ê´€ í•„ìš”."
            )

        print(f">> ARES ì‹¬ì‚¬ê´€ ë¡œë“œ ì™„ë£Œ. ì´ {len(self.judges)}ê°œ ëª¨ë¸ í™œì„±í™”.")


    def evaluate_triple(self, query: str, context: str, answer: str) -> Dict[str, Dict[str, float | int]]:
        """
        í•˜ë‚˜ì˜ Q-C-A ìŒì— ëŒ€í•´ CR, AF, AR ì ìˆ˜(0 ë˜ëŠ” 1)ì™€ Softmax í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        ë°˜í™˜ í˜•ì‹ ë³€ê²½: { 'contextrelevance': {'machine_pred': 0/1, 'prob_neg': 0.xx, 'prob_pos': 0.yy}, ... }
        """
        # ë°˜í™˜ íƒ€ì…ì´ ë³€ê²½ë˜ë¯€ë¡œ ë”•ì…”ë„ˆë¦¬ì˜ ê°’ë„ ë”•ì…”ë„ˆë¦¬ê°€ ë©ë‹ˆë‹¤.
        results: Dict[str, Dict[str, float | int]] = {}

        judge_inputs = {
            JUDGE_PREDICTION_FIELDS[KEY_CR]: (query, context, self.judges[KEY_CR]),
            JUDGE_PREDICTION_FIELDS[KEY_AF]: (context, answer, self.judges[KEY_AF]),
            JUDGE_PREDICTION_FIELDS[KEY_AR]: (query, answer, self.judges[KEY_AR]),
        }

        with torch.no_grad():
            for name, (a, b, model) in judge_inputs.items():
                inputs = self.tokenizer(
                    a, b,
                    return_tensors="pt",
                    truncation=True,
                    padding="max_length",
                    max_length=self.max_length,
                ).to(self.device)

                outputs = model.forward(**inputs)
                logits = outputs.logits

                # 1. Softmax ì ìš©í•˜ì—¬ í™•ë¥  íšë“
                probabilities = torch.softmax(logits, dim=1).squeeze().cpu().numpy()

                # 2. í´ë˜ìŠ¤ ì˜ˆì¸¡ (argmax)
                prediction = int(torch.argmax(logits, dim=1).item())

                # 3. ê¸ì •(1) ë° ë¶€ì •(0) í™•ë¥  ì €ì¥ (ì†Œìˆ˜ì  4ì§¸ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼)
                prob_neg = round(float(probabilities[0]), 4)  # ë¶€ì • í™•ë¥  (Class 0)
                prob_pos = round(float(probabilities[1]), 4)  # ê¸ì • í™•ë¥  (Class 1)

                # 4. ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì— Softmax í™•ë¥ ê³¼ ì˜ˆì¸¡ í´ë˜ìŠ¤ ëª¨ë‘ ì €ì¥
                results[name] = {
                    'machine_pred': prediction,
                    'prob_neg': prob_neg,
                    'prob_pos': prob_pos
                }

        return results


# ===================================================================
# 3. PPI í†µê³„ ë° ë³´ì • ê³„ì‚° í´ë˜ìŠ¤
# ===================================================================

class PPICalculator:
    """PPI ë³´ì • ë° í†µê³„ ê³„ì‚°ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤."""

    def __init__(self, ci_z_score: float = CI_Z_SCORE):
        self.ci_z_score = ci_z_score

    def calculate_accuracy(self, stats: Dict[str, Any]) -> str:
        """rectifier_termsë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¬ì‚¬ê´€ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³  ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        terms = stats.get('rectifier_terms')
        labeled_n = stats.get('labeled_n')

        if not terms or labeled_n == 0:
            return 'N/A'

        # ì˜¤ë¥˜ íšŸìˆ˜ = (rectifier_terms ë¦¬ìŠ¤íŠ¸ì—ì„œ 0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜)
        error_count = sum(1 for term in terms if term != 0.0)
        accuracy = 1.0 - (error_count / labeled_n)
        return f"{accuracy:13.3f}"

    def generate_golden_set_stat(self, golden_set_stats: Dict[str, Dict[str, Any]]) -> str:
        """ê³¨ë“ ì…‹ í†µê³„ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not golden_set_stats:
            print("[FAIL] ê³¨ë“ ì…‹ í‰ê°€ ê²°ê³¼ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return "ê³¨ë“ ì…‹ í‰ê°€ ê²°ê³¼ ì—†ìŒ"

        # ... (ê¸°ì¡´ generate_golden_set_stat ë¡œì§ ìœ ì§€)
        cr_mean = golden_set_stats.get(KEY_CR, {}).get('machine_mean', 'N/A')
        af_mean = golden_set_stats.get(KEY_AF, {}).get('machine_mean', 'N/A')
        ar_mean = golden_set_stats.get(KEY_AR, {}).get('machine_mean', 'N/A')

        cr_acc = self.calculate_accuracy(golden_set_stats.get(KEY_CR, {}))
        af_acc = self.calculate_accuracy(golden_set_stats.get(KEY_AF, {}))
        ar_acc = self.calculate_accuracy(golden_set_stats.get(KEY_AR, {}))

        header = f"| {'êµ¬ë¶„':^12} | {'CR':^13} | {'AF':^13} | {'AR':^13} |"
        separator = f"+----------------+---------------+---------------+---------------+"
        mean_row = (
            f"| {'ì˜ˆì¸¡í‰ê· ':^12} | {cr_mean:>13.3f} | {af_mean:>13.3f} | {ar_mean:>13.3f} |"
            f"  ì‹¬ì‚¬ê´€ì´ 1ì´ë¼ê³  ì˜ˆì¸¡í•œ ë¹„ìœ¨ (ê¸ì • í¸í–¥)"
        )
        acc_row = (
            f"| {'ì •ë‹µë¹„ìœ¨':^12} | {cr_acc:>13} | {af_acc:>13} | {ar_acc:>13} |"
            f"  ì‹¬ì‚¬ê´€ ì˜ˆì¸¡ì˜ ì •í™•ë„"
        )
        markdown_content = "\n".join([header, separator, mean_row, acc_row])
        markdown_string = f"```\n{markdown_content}\n```"
        return markdown_string

    def evaluate_golden_set(self, judge: AresJudge, golden_set_filepath: str) -> Dict[str, Dict[str, Any]]:
        """
        íŠ¹ì • ê³¨ë“ ì…‹ íŒŒì¼ì„ ì‹¬ì‚¬ê´€ì´ í‰ê°€í•˜ê³ , PPI í¸í–¥ ê³„ì‚°ì— í•„ìš”í•œ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        golden_stats = {key: {'labeled_n': 0, 'rectifier_terms': [], 'machine_mean_sum': 0.0} for key in JUDGE_TYPES}
        # ì™¸ë¶€ ëª¨ë“ˆì˜ _load_json_lines í•¨ìˆ˜ ì‚¬ìš© ê°€ì •
        golden_records = _load_json_lines(golden_set_filepath)

        if not golden_records:
            print(f"[WARN] ê³¨ë“ ì…‹ íŒŒì¼ {golden_set_filepath}ì— í‰ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        print(f"\n>> ê³¨ë“ ì…‹ í‰ê°€ ì‹œì‘. {golden_set_filepath}, ì´ {len(golden_records)}ê°œ ìƒ˜í”Œ ì‹¬ì‚¬ê´€ ì˜ˆì¸¡ ì¤‘...")

        # ğŸš¨ ìˆ˜ì •: pbar ê°ì²´ í• ë‹¹ ë° ëª…ì‹œì  ì œì–´
        pbar = tqdm(golden_records, desc="ê³¨ë“ ì…‹ ì‹¬ì‚¬ê´€ í‰ê°€ ì¤‘")

        for data in pbar:
            try:
                # Q, C, A ì¶”ì¶œ ë° ì •ê·œí™”
                query = ' '.join(data.get('q', '').split()).strip()
                context = ' '.join(data.get('c', '').split()).strip()
                answer = ' '.join(data.get('a', '').split()).strip()

                if not all([query, context, answer]):
                    continue

                # 1. LM ì‹¬ì‚¬ê´€ ì˜ˆì¸¡ (Yhat_labeled) - ë”•ì…”ë„ˆë¦¬ in ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜
                scores_with_probs = judge.evaluate_triple(query, context, answer)

                # 2. LM ì˜ˆì¸¡ê°’ê³¼ ì¸ê°„ ì£¼ì„ê°’ ë¹„êµ
                for axis in JUDGE_TYPES:
                    pred_key = JUDGE_PREDICTION_FIELDS[axis]
                    axis_scores = scores_with_probs.get(pred_key)

                    if axis_scores is None: continue

                    # AresJudge.evaluate_triple ë³€ê²½ ì‚¬í•­ ë°˜ì˜: 'machine_pred' í‚¤ì—ì„œ ì˜ˆì¸¡ í´ë˜ìŠ¤ ì¶”ì¶œ
                    machine_pred = axis_scores.get('machine_pred')

                    gold_key = GOLD_LABEL_FIELDS[axis]
                    gold_label = data.get(gold_key)

                    if machine_pred is None or gold_label is None:
                        continue

                    # í†µê³„ ì—…ë°ì´íŠ¸
                    machine_pred = int(machine_pred)
                    gold_label = int(gold_label)

                    rectifier_term = float(machine_pred - gold_label)

                    stats = golden_stats[axis]
                    stats['labeled_n'] += 1
                    stats['rectifier_terms'].append(rectifier_term)
                    stats['machine_mean_sum'] += machine_pred

            except Exception:
                continue

        # ğŸš¨ ìˆ˜ì •: tqdm ëª…ì‹œì  ì¢…ë£Œ ë° ë‹¤ìŒ ë¡œê·¸ë¥¼ ìœ„í•œ ì¤„ë°”ê¿ˆ ì¶”ê°€
        pbar.close()
        print()  # ë‹¤ìŒ ë¡œê·¸ê°€ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ê°•ì œ ì¤„ë°”ê¿ˆ

        # 3. ìµœì¢… í†µê³„ ê³„ì‚°
        final_golden_stats = {}
        for axis, stats in golden_stats.items():
            if stats['labeled_n'] > 0:
                final_golden_stats[axis] = {
                    'labeled_n': stats['labeled_n'],
                    'rectifier_terms': stats['rectifier_terms'],
                    'machine_mean': stats['machine_mean_sum'] / stats['labeled_n']
                }

        return final_golden_stats


    def calculate_ppi_asymptotic_ci(
            self,
            machine_preds: List[int],
            rectifiers: List[float],
            total_n: int,
            labeled_n: int
    ) -> float:
        """PPI Asymptotic CI Half-width ê³„ì‚°."""
        if labeled_n <= 1 or total_n <= 0:
            return 0.0

        y_hat_array: NDArray[np.float64] = np.array(machine_preds, dtype=np.float64)
        rectifier_array: NDArray[np.float64] = np.array(rectifiers, dtype=np.float64)

        sigma2_f: float = float(np.var(y_hat_array))
        sigma2_rec: float = float(np.var(rectifier_array))

        variance: float = (sigma2_f / float(total_n)) + (sigma2_rec / float(labeled_n))
        half_width: float = self.ci_z_score * math.sqrt(variance)
        return round(half_width, 3)

    # PPICalculator í´ë˜ìŠ¤ ë‚´ë¶€ì˜ calculate_ppi_summary ë©”ì„œë“œ

    def calculate_ppi_summary(
            self,
            file_base_name: str,
            current_lm_scores: Dict[str, List[int]],
            total_n: int,
            golden_set_stats: Dict[str, Dict[str, Any]],
            golden_set_name: str
    ) -> Dict[str, Any]:
        """
        LM ì˜ˆì¸¡ ê²°ê³¼ì™€ ê³¨ë“ ì…‹ í†µê³„ë¥¼ ê²°í•©í•˜ì—¬ PPI ìš”ì•½ ê²°ê³¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        (ê³¨ë“ ì…‹ í†µê³„ê°€ ëˆ„ë½ë˜ë©´ ëª…ì‹œì  ì˜¤ë¥˜ ë°œìƒ)
        """

        summary: Dict[str, Any] = {
            "model_name": file_base_name,
            "golden_set_name": golden_set_name,
            "n": total_n,
            "ppi_active": True,
            # CR ê¸°ì¤€ìœ¼ë¡œ ëŒ€í‘œê°’ ì‚¬ìš©. í†µê³„ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ê¸°ë³¸ê°’ 0 ì„¤ì •
            "labeled_n_rep": golden_set_stats.get(KEY_CR, {}).get('labeled_n', 0),
        }

        overall_corrected_scores: List[float] = []

        for axis in JUDGE_TYPES:
            # 1. LM ì˜ˆì¸¡ ê²°ê³¼ (Yhat_unlabeled)
            scores: List[int] = current_lm_scores[axis]

            # 2. ê³¨ë“ ì…‹ í†µê³„ (Rectifier Terms, labeled_n)
            golden_axis_stats = golden_set_stats.get(axis, {})

            labeled_n_axis: int = golden_axis_stats.get('labeled_n', 0)

            # ğŸš¨ í•µì‹¬ ìˆ˜ì •: í•„ìˆ˜ í†µê³„ ëˆ„ë½ ì‹œ ëª…ì‹œì  ì˜¤ë¥˜ ë°œìƒ
            if labeled_n_axis == 0 or not golden_axis_stats:
                raise RuntimeError(
                    f"[{axis} ì¶• í†µê³„ ëˆ„ë½] ê³¨ë“ ì…‹ '{golden_set_name}'ì— '{axis}' ì¶•ì˜ ìœ íš¨í•œ ë¼ë²¨ë§ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. "
                    f"Labeled N: {labeled_n_axis}."
                )

            rectifier_terms: List[float] = golden_axis_stats['rectifier_terms']

            # 3. í†µê³„ ê³„ì‚°
            machine_mean: float = sum(scores) / float(total_n)

            # 3-2. í¸í–¥ (Rectifier) = Avg(Yhat_labeled - Y_labeled)
            # labeled_n_axisëŠ” 0ì´ ì•„ë‹˜ì„ ìœ„ì—ì„œ ë³´ì¥í–ˆìŒ
            rectifier: float = sum(rectifier_terms) / labeled_n_axis

            # 3-3. ë³´ì •ëœ ì„±ëŠ¥ (Corrected Mean)
            corrected_mean: float = max(0.0, min(1.0, machine_mean - rectifier))

            # 3-4. ì‹ ë¢° êµ¬ê°„ (CI)
            margin: float = self.calculate_ppi_asymptotic_ci(scores, rectifier_terms, total_n, labeled_n_axis)

            # 4. ìµœì¢… ìš”ì•½ì— ì¶”ê°€
            summary[axis] = {
                "machine_mean": round(machine_mean, 2),
                "corrected_mean": round(corrected_mean, 2),
                "applied_rectifier": round(rectifier, 3),
                "ci": round(margin, 2)
            }
            overall_corrected_scores.append(corrected_mean)

        if overall_corrected_scores:
            summary["overall"] = round(sum(overall_corrected_scores) / len(overall_corrected_scores), 2)

        return summary


# ===================================================================
# 4. ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜
# ===================================================================
# ì´ˆê¸°í™”
def _load_judges_and_calc() -> Tuple[AresJudge, PPICalculator]:
    """ì‹¬ì‚¬ê´€ê³¼ ê³„ì‚°ê¸° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë¡œë“œí•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    # í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±ì€ ë©”ì¸ íŒŒì´í”„ë¼ì¸ì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    try:
        judge = AresJudge()
        calculator = PPICalculator()
        return judge, calculator
    except RuntimeError as e:
        print(f"\n[FATAL ERROR] ARES ì‹¬ì‚¬ê´€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise


# ê³¨ë“ ì…‹ í‰ê°€
def _evaluate_golden_sets(judge: AresJudge, calculator: PPICalculator) -> Tuple[Dict, Dict]:
    """
    ê³¨ë“ ì…‹ ë””ë ‰í† ë¦¬ë¥¼ íƒìƒ‰í•˜ê³ , ê° ê³¨ë“ ì…‹ì„ í‰ê°€í•˜ì—¬ í†µê³„ ë§µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ê³¨ë“ ì…‹ í†µê³„ê°€ ì—†ìœ¼ë©´ Fatal Error ë°œìƒ)
    """
    GOLDEN_DIR = config.DATA_GOLDEN_DIR

    # 1. ê³¨ë“  ë¼ë²¨ íŒŒì¼ ê²€ìƒ‰ (ë‹¤ì¤‘ ê³¨ë“ ì…‹ì„ ì²˜ë¦¬í•˜ë„ë¡ í™•ì¥)
    golden_files: List[Tuple[str, str]] = []
    if os.path.isdir(GOLDEN_DIR):
        for filename in os.listdir(GOLDEN_DIR):
            if filename.endswith('.jsonl'):
                golden_name = filename.replace('.jsonl', '')
                file_path = os.path.join(GOLDEN_DIR, filename)
                golden_files.append((golden_name, file_path))

        print(f"\n[INFO] {GOLDEN_DIR}ì—ì„œ ì´ {len(golden_files)}ê°œì˜ ê³¨ë“ ì…‹ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    else:
        print(f"\n[WARN] ê³¨ë“ ì…‹ ë””ë ‰í† ë¦¬ '{GOLDEN_DIR}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    if not golden_files:
        raise RuntimeError("PPI ë³´ì •ì„ ìœ„í•œ ê³¨ë“  ë¼ë²¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # 2. ëª¨ë“  ê³¨ë“ ì…‹ í‰ê°€ ë° í†µê³„ ì €ì¥ (ë©”ëª¨ë¦¬ ì²˜ë¦¬)
    golden_stats_map: Dict[str, Dict[str, Dict[str, Any]]] = {}
    golden_markdown_map: Dict[str, str] = {}

    for golden_name, path in golden_files:
        try:
            stats = calculator.evaluate_golden_set(judge, path)
            if stats:
                golden_stats_map[golden_name] = stats
                golden_markdown_map[golden_name] = calculator.generate_golden_set_stat(stats)
                print(f"\n   [SUCCESS] ê³¨ë“ ì…‹ '{golden_name}' í‰ê°€ ì™„ë£Œ.")
            else:
                print(f"\n   [WARN] ê³¨ë“ ì…‹ '{golden_name}' í‰ê°€ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            print(f"\n[FATAL ERROR] ê³¨ë“ ì…‹ '{golden_name}' í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê±´ë„ˆëœë‹ˆë‹¤.")

    if not golden_stats_map:
        raise RuntimeError("PPI ë³´ì •ì„ ìœ„í•œ ìœ íš¨í•œ ê³¨ë“  ë¼ë²¨ ë°ì´í„° í‰ê°€ ì‹¤íŒ¨.")

    return golden_stats_map, golden_markdown_map


# ëŒ€ê·œëª¨ í‰ê°€ ë£¨í”„
def _process_input_files(judge: AresJudge, calculator: PPICalculator, golden_stats_map: Dict) -> Tuple[
    List, int, float]:
    """
    ì…ë ¥ íŒŒì¼ì„ ìˆœíšŒí•˜ë©° LM ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , PPI ë³´ì •ì„ ì ìš©í•˜ì—¬ model_summariesë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    INPUT_DIR = config.DATA_IN_DIR
    input_files = [
        os.path.join(INPUT_DIR, f) for f in os.listdir(INPUT_DIR) if f.endswith('.jsonl')
    ]

    if not input_files:
        print(f"\n[WARN] QCA `.jsonl` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return [], 0, 0.0

    print(f"\n[INFO] í‰ê°€ ëŒ€ìƒ íŒŒì¼ ê°¯ìˆ˜ : {len(input_files)}")

    total_successful_evals = 0
    model_summaries: List[Dict[str, Any]] = []
    full_start_time = time.time()

    # 2-2. íŒŒì¼ë³„ í‰ê°€, LM ì˜ˆì¸¡ íŒŒì¼ ìƒì„± ë° ê²°ê³¼ ì§‘ê³„
    for file_path in input_files:
        start_time = time.time()
        file_base_name = os.path.basename(file_path).replace('.jsonl', '')

        # QCA í‰ê°€ (Judge)ëŠ” í•œ ë²ˆë§Œ ìˆ˜í–‰
        print(f"\n--- ëŒ€ê·œëª¨ í‰ê°€ ì‹œì‘: {file_base_name} ---")
        current_lm_scores = {k: [] for k in JUDGE_TYPES}
        # all_results_for_file = [] # ì €ì¥ ë¡œì§ ë¹„í™œì„±í™” ì‹œ ë¶ˆí•„ìš”
        processed_count_in_file = 0

        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            for line in tqdm(lines, desc=f"í‰ê°€ ì¤‘ [{file_base_name}]"):
                try:
                    data = json.loads(line.strip())
                    query = ' '.join(data.get('q', '').split()).strip()
                    context = ' '.join(data.get('c', '').split()).strip()
                    answer = ' '.join(data.get('a', '').split()).strip()

                    if not all([query, context, answer]): continue

                    scores_with_probs = judge.evaluate_triple(query, context, answer)

                    # all_results_for_file.append({**data, **scores_with_probs}) # ì €ì¥ ë¹„í™œì„±í™”

                    for axis in JUDGE_TYPES:
                        pred_key = JUDGE_PREDICTION_FIELDS[axis]
                        axis_scores = scores_with_probs.get(pred_key)

                        if axis_scores is None: continue

                        current_lm_scores[axis].append(axis_scores.get('machine_pred', 0))

                    total_successful_evals += 1
                    processed_count_in_file += 1

                except Exception as e:
                    print(f"[ERROR] í‰ê°€ ì¤‘ ì˜ˆì™¸ë°œìƒ : {e}")
                    continue

        end_time = time.time()
        print(f"[INFO] í‰ê°€ ì†Œìš”ì‹œê°„ : {end_time - start_time:,.2f}ì´ˆ ")

        # 2-3. í‰ê°€ì…‹ë‹¹ ëª¨ë“  ê³¨ë“ ì…‹ì— ëŒ€í•´ PPI í†µê³„ ê³„ì‚° ë° ì§‘ê³„
        if processed_count_in_file > 0:
            for golden_name, golden_stats in golden_stats_map.items():
                summary = calculator.calculate_ppi_summary(
                    file_base_name,
                    current_lm_scores,
                    processed_count_in_file,
                    golden_stats,
                    golden_name
                )
                model_summaries.append(summary)

            print(f"   [ì§‘ê³„ ì™„ë£Œ] '{file_base_name}' ê²°ê³¼ ì§‘ê³„ ì™„ë£Œ. (ëª¨ë“  {len(golden_stats_map)}ê°œ ê³¨ë“ ì…‹ ì ìš©)")
        else:
            print(f"   [ERROR] ì‹¬ì‚¬ê´€ì˜ í‰ê°€ ê²°ê³¼ ì—†ìŒ - íŒŒì¼: {file_base_name}")

    full_elapsed_time = time.time() - full_start_time
    return model_summaries, total_successful_evals, full_elapsed_time


# ë³´ê³ ì„œ ë° ìš”ì•½
def _generate_report_and_summary(golden_markdown_map: Dict, model_summaries: List, total_successful_evals: int,
                                 full_elapsed_time: float) -> None:
    """
    ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ì½˜ì†”ì— ì‹¤í–‰ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    REPORT_DIR = config.DATA_REPORT_DIR
    MODEL_NAME = config.MODEL_NAME

    # ğŸš¨ ë””ë²„ê¹…ì„ ìœ„í•´ ì´ ë¶€ë¶„ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. (ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ì œê±° ê°€ëŠ¥)
    if model_summaries:
        print(f"\n[DEBUG] Model Summaries ì²« ë²ˆì§¸ ìš”ì†Œ: {model_summaries[0]}")

    if model_summaries:
        report_content: str = report_util.generate_summary_report(golden_markdown_map, model_summaries)
        timestamp: str = time.strftime("%Y%m%d_%H%M%S")

        # íŒŒì¼ëª… ìˆ˜ì • ë°˜ì˜
        report_filename: str = f"{MODEL_NAME}_{timestamp}.md"
        output_path: str = os.path.join(REPORT_DIR, report_filename)

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        print(f"\n[ìµœì¢… ì™„ë£Œ] ARES í†µê³„ ë³´ê³ ì„œ ìƒì„±ë¨ â†’ {output_path}")

    print("\n\n=============== LM ì˜ˆì¸¡ ìƒì„± ìµœì¢… ìš”ì•½ ===============")
    print(f"ì´ LM ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜: {total_successful_evals}ê°œ")
    print(f"ì´ ì†Œìš” ì‹œê°„: {full_elapsed_time:.2f}ì´ˆ")
    print("==================================================")



# ì‹¤í–‰
def run_ares_pipeline():
    """
    ARES ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ê´€ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    """
    try:
        # 0. í™˜ê²½ ì„¤ì •
        os.makedirs(config.DATA_IN_DIR, exist_ok=True)
        os.makedirs(config.DATA_REPORT_DIR, exist_ok=True)
        print(f"\n[SETUP] í‰ê°€ëŒ€ìƒì¸ QCA ì…ë ¥ ë””ë ‰í† ë¦¬: {config.DATA_IN_DIR}, ë³´ê³ ì„œ ë””ë ‰í† ë¦¬: {config.DATA_REPORT_DIR}")

        # 1. ì´ˆê¸°í™” ë° ê³¨ë“ ì…‹ í‰ê°€
        judge, calculator = _load_judges_and_calc()
        golden_stats_map, golden_markdown_map = _evaluate_golden_sets(judge, calculator)

        # 2. ëŒ€ê·œëª¨ í‰ê°€ ì‹¤í–‰
        model_summaries, total_successful_evals, full_elapsed_time = _process_input_files(
            judge, calculator, golden_stats_map
        )

        # 3. ë³´ê³ ì„œ ìƒì„± ë° ìµœì¢… ìš”ì•½
        _generate_report_and_summary(golden_markdown_map, model_summaries, total_successful_evals, full_elapsed_time)

    except RuntimeError as e:
        print(f"\n[FATAL ERROR] íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return


if __name__ == "__main__":
    run_ares_pipeline()

