import json
from transformers import AutoTokenizer, DistilBertForSequenceClassification, AutoModelForSequenceClassification, \
    pipeline
import numpy as np
from datasets import Dataset
from sklearn.metrics import f1_score
import os
import logging
import config

# --- ë¡œê¹… ì„¤ì •: í† í°í™” ê²½ê³  ë©”ì‹œì§€ (Warning) ìˆ¨ê¸°ê¸° ---
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)

# ëª¨ë¸ ì„¤ì • (ì „ì—­ ìƒìˆ˜ë¼ê³  ê°€ì •)
NUM_TRAIN_EPOCHS = 5
TRAIN_BATCH_SIZE = 32
EVAL_BATCH_SIZE = 64
MAX_SEQ_LENGTH = 512

JUDGE_TASKS = [
    ('context_relevance', 'q', 'c', 'L_CR'),
    ('answer_faithfulness', 'c', 'a', 'L_AF'),
    ('answer_relevance', 'q', 'a', 'L_AR'),
]


def load_jsonl_to_list(file_path):
    """ì§€ì •ëœ JSONL íŒŒì¼ ê²½ë¡œë¡œë¶€í„° ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    all_data = []
    if not os.path.exists(file_path):
        file_name = os.path.basename(file_path)
        print(f"[ERROR] íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_name} at {file_path}")
        return []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                all_data.append(json.loads(line))
        file_name = os.path.basename(file_path)
        print(f"[INFO] {file_name} íŒŒì¼ì—ì„œ ì´ {len(all_data)}ê°œì˜ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ.")
        return all_data
    except Exception as e:
        print(f"[ERROR] JSONL íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


def prepare_dataset(data_list, text_a_key, text_b_key, label_key):
    """
    ë¡œë“œëœ ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ Hugging Face Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    (pandas ì‚¬ìš© ì—†ì´, Dataset.from_dict ì‚¬ìš©)
    """

    dataset_dict = {
        'text_a': [],
        'text_b': [],
        'labels': []
    }

    for item in data_list:
        dataset_dict['text_a'].append(item.get(text_a_key))
        dataset_dict['text_b'].append(item.get(text_b_key))
        dataset_dict['labels'].append(item.get(label_key))

    return Dataset.from_dict(dataset_dict)


# ì„±ëŠ¥ ì§€í‘œ í•¨ìˆ˜ (F1 ìŠ¤ì½”ì–´ ë° ì •í™•ë„) - ìœ ì§€
def compute_metrics(y_true, y_pred):
    """ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸”ì„ ê¸°ë°˜ìœ¼ë¡œ F1 ë° Accuracyë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    accuracy = np.mean(y_true == y_pred)
    # ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ 'binary' average ì‚¬ìš©
    f1 = f1_score(y_true, y_pred, average='binary')

    return {
        'accuracy': accuracy,
        'f1': f1,
    }


def evaluate_judges():
    """
    config ëª¨ë“ˆì˜ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬, ëª¨ë¸ ë””ë ‰í† ë¦¬ ë‚´ì˜ ì„¸ ê°€ì§€ Judge ëª¨ë¸ì„ ì°¾ì•„
    pipeline ëª¨ë“œë¡œ ì„±ëŠ¥ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    (CPU Only í™˜ê²½ì—ì„œ accelerate ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ ë°©ì‹)
    """
    print("\n\n=============================================")
    print("âœ¨ ì‹¬ì‚¬ê´€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘ (Pipeline ëª¨ë“œ) âœ¨")
    print("=============================================")

    all_results = []

    # ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ í´ë˜ìŠ¤ ê²°ì • (Pipelineì€ ë‚´ë¶€ì ìœ¼ë¡œ AutoModelì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì •ë³´ ì œê³µ ëª©ì )
    model_name_lower = config.MODEL_NAME.lower()
    if "distil" in model_name_lower:
        MODEL_CLS = DistilBertForSequenceClassification
        print(f"[INFO] ëª¨ë¸ í´ë˜ìŠ¤: DistilBertForSequenceClassification")
    elif "bert" in model_name_lower or "klue" in model_name_lower:
        MODEL_CLS = AutoModelForSequenceClassification
        print(f"[INFO] ëª¨ë¸ í´ë˜ìŠ¤: AutoModelForSequenceClassification")
    else:
        MODEL_CLS = AutoModelForSequenceClassification
        print(f"[INFO] ëª¨ë¸ í´ë˜ìŠ¤: AutoModelForSequenceClassification (ê¸°ë³¸ê°’ ì„¤ì •)")

    # 1. í‰ê°€ìš© íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° í™•ì¸ (ëª¨ë“  Judgeê°€ ë™ì¼í•œ íŒŒì¼ì„ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •)
    eval_json_path = os.path.join(config.DATA_EVAL_DIR, config.DATA_EVAL_FILE_NAME)
    if not os.path.exists(eval_json_path):
        print(f"[ERROR] í‰ê°€ì— ì‚¬ìš©ë  íŒŒì¼ {eval_json_path}ê°€ ì—†ì–´ì„œ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ (ì „ì²´ ì›ë³¸)
    raw_eval_data_all = load_jsonl_to_list(eval_json_path)

    for judge_type, text_a_key, text_b_key, label_key in JUDGE_TASKS:
        # 3. ëª¨ë¸ ê²½ë¡œ ìƒì„± ë° í™•ì¸ (ê·œì¹™: config.MODEL_DIR/judge_type)
        model_path = os.path.join(config.MODEL_DIR, judge_type)
        if not os.path.exists(model_path):
            print(f"[SKIP] ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœ±ë‹ˆë‹¤: {model_path}")
            continue

        print(f"\n--- {judge_type} Judge í‰ê°€ ---")
        print(f"[INFO] ëª¨ë¸ ê²½ë¡œ: {model_path}")

        try:
            # 4. Pipeline ë¡œë“œ (CPU ëª…ì‹œ: accelerate ì˜¤ë¥˜ íšŒí”¼)
            qa_pipeline = pipeline(
                "text-classification",
                model=model_path,
                tokenizer=model_path,
                device=-1,  # CPU Only í™˜ê²½ ëª…ì‹œ
            )

            # 5. í‰ê°€ ë°ì´í„° ì¤€ë¹„ ë° í•„í„°ë§
            eval_dataset_full = prepare_dataset(raw_eval_data_all, text_a_key, text_b_key, label_key)

            # í•„í„°ë§ ë¡œì§ (ìœ íš¨í•˜ì§€ ì•Šì€ ë¬¸ìì—´ ì œê±°)
            def is_valid_text(example):
                text_a_val = example.get('text_a')
                text_b_val = example.get('text_b')
                return isinstance(text_a_val, str) and isinstance(text_b_val,
                                                                  str) and text_a_val.strip() != "" and text_b_val.strip() != ""

            # í•„í„°ë§ëœ ë°ì´í„°ì…‹ (ìœ íš¨í•œ ë°ì´í„°ë§Œ ë‚¨ìŒ)
            eval_dataset = eval_dataset_full.filter(is_valid_text)

            initial_eval_size = len(eval_dataset_full)
            print(
                f"[INFO] í‰ê°€ ë°ì´í„° í•„í„°ë§: {initial_eval_size}ê°œ -> {len(eval_dataset)}ê°œ (ì œê±°ë¨: {initial_eval_size - len(eval_dataset)}ê°œ)")

            # 6. ì˜ˆì¸¡ ì…ë ¥ ìƒì„± ë° ì˜ˆì¸¡ ì‹¤í–‰
            test_texts = [
                (row['text_a'], row['text_b'])
                for row in eval_dataset
            ]

            # ì˜ˆì¸¡ ìˆ˜í–‰ (Pipeline)
            print(f"[INFO] {judge_type} ì˜ˆì¸¡ ì‹œì‘...")
            predictions = qa_pipeline(
                test_texts,
                truncation=True,
                padding='max_length',
                max_length=MAX_SEQ_LENGTH,
                batch_size=EVAL_BATCH_SIZE  # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì‚¬ìš©
            )

            # 7. ì˜ˆì¸¡ ê²°ê³¼ì™€ ë ˆì´ë¸” ê°œìˆ˜ ë³´ì • (í•µì‹¬ ìˆ˜ì •)

            # ì˜ˆì¸¡ ê²°ê³¼ (y_pred) ìƒì„±
            y_pred_labels = [int(p['label'].split('_')[-1]) for p in predictions]
            y_pred = np.array(y_pred_labels)

            # ì •ë‹µ ë ˆì´ë¸” (y_true) ì¶”ì¶œ
            y_true_full = np.array(eval_dataset['labels'])

            # ğŸŒŸ [ë ˆì´ë¸” ì¬êµ¬ì„± ë¡œì§: ê°œìˆ˜ ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°] ğŸŒŸ
            len_true = len(y_true_full)
            len_pred = len(y_pred)

            min_len = min(len_true, len_pred)

            # ë‘ ë°°ì—´ì˜ ê¸¸ì´ë¥¼ ë” ì‘ì€ ìª½ì— ë§ì¶° ìë¦…ë‹ˆë‹¤.
            y_true_adjusted = y_true_full[:min_len]
            y_pred_adjusted = y_pred[:min_len]

            if len_true != len_pred:
                print(f"[WARN] ì˜ˆì¸¡/ë ˆì´ë¸” ê°œìˆ˜ ë¶ˆì¼ì¹˜ ë°œìƒ ({len_true} vs {len_pred}). {min_len}ê°œì— ë§ì¶° í‰ê°€í•©ë‹ˆë‹¤.")

            # 8. ì§€í‘œ ê³„ì‚°
            metrics = compute_metrics(y_true_adjusted, y_pred_adjusted)

            result = {
                'Judge': judge_type,
                'F1-Score': metrics.get('f1'),
                'Accuracy': metrics.get('accuracy'),
                'Path': model_path
            }
            all_results.append(result)

            print(f"[RESULT] {judge_type} F1 Score: {result['F1-Score']:.4f}, Accuracy: {result['Accuracy']:.4f}")

        except Exception as e:
            print(f"[ERROR] {judge_type} ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            all_results.append({'Judge': judge_type, 'Error': str(e), 'Path': model_path})

    # 9. ìµœì¢… ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n=============================================")
    print("âœ… ë””ë ‰í† ë¦¬ í‰ê°€ ìµœì¢… ìš”ì•½")
    print("=============================================")
    for res in all_results:
        if 'F1-Score' in res:
            print(f"[{res['Judge']}]: F1 Score = {res['F1-Score']:.4f}, Accuracy = {res['Accuracy']:.4f}")
        else:
            print(f"[{res['Judge']}]: í‰ê°€ ì‹¤íŒ¨ - {res['Error']}")
    print("=============================================")

    return all_results


if __name__ == "__main__":
    final_metrics = evaluate_judges()

    if final_metrics:
        print("\n[SUCCESS] ë‹¨ì¼ ëª¨ë¸ í‰ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ.")