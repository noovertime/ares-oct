import json
import torch
from transformers import AutoTokenizer, DistilBertForSequenceClassification, AutoModelForSequenceClassification, \
    TrainingArguments, Trainer
import numpy as np
from datasets import Dataset
from sklearn.metrics import f1_score
import os
import logging
# âš ï¸ NOTE: config ëª¨ë“ˆì´ ì™¸ë¶€ì— ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
import config
from torch.utils.data import DataLoader, SequentialSampler, TensorDataset

# --- ë¡œê¹… ì„¤ì •: í† í°í™” ê²½ê³  ë©”ì‹œì§€ (Warning) ìˆ¨ê¸°ê¸° ---
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)

# ===================================================================
# 0. ì „ì—­ ìƒìˆ˜ ì„¤ì • (config ëª¨ë“ˆì—ì„œ ê°€ì ¸ì˜¨ë‹¤ê³  ê°€ì •)
# ===================================================================
# âš ï¸ NOTE: config.MODEL_NAME ë“±ì€ ì™¸ë¶€ config íŒŒì¼ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
# ì˜ˆì‹œ:
# MODEL_NAME = "monologg/distilkobert"
# DATA_EVAL_DIR = "/data/eval"
# DATA_EVAL_FILE_NAME = "eval_data.jsonl"
# MODEL_DIR = "/models"

NUM_TRAIN_EPOCHS = 5
TRAIN_BATCH_SIZE = 32
EVAL_BATCH_SIZE = 64
MAX_SEQ_LENGTH = 512

JUDGE_TASKS = [
    ('context_relevance', 'q', 'c', 'L_CR'),
    ('answer_faithfulness', 'c', 'a', 'L_AF'),
    ('answer_relevance', 'q', 'a', 'L_AR'),
]


# ===================================================================
# 1. ë³´ì¡° í•¨ìˆ˜
# ===================================================================

def load_jsonl_to_list(file_path):
    """ì§€ì •ëœ JSONL íŒŒì¼ ê²½ë¡œë¡œë¶€í„° ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    all_data = []
    if not os.path.exists(file_path):
        file_name = os.path.basename(file_path)
        print(f"[ERROR] íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_name} at {file_path}")
        return []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                all_data.append(json.loads(line))
        file_name = os.path.basename(file_path)
        print(f"[INFO] {file_name} íŒŒì¼ì—ì„œ ì´ {len(all_data)}ê°œì˜ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ.")
        return all_data
    except Exception as e:
        print(f"[ERROR] JSONL íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


def prepare_dataset(data_list, text_a_key, text_b_key, label_key):
    """
    ë¡œë“œëœ ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ Hugging Face Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    (pandas ì‚¬ìš© ì—†ì´, Dataset.from_dict ì‚¬ìš©)
    """
    dataset_dict = {
        'text_a': [],
        'text_b': [],
        'labels': []
    }
    for item in data_list:
        # NOTE: .get()ì„ ì‚¬ìš©í•˜ì—¬ í‚¤ê°€ ì—†ì„ ë•Œ ì˜¤ë¥˜ ë°©ì§€
        dataset_dict['text_a'].append(item.get(text_a_key))
        dataset_dict['text_b'].append(item.get(text_b_key))
        dataset_dict['labels'].append(item.get(label_key))

    return Dataset.from_dict(dataset_dict)


def compute_metrics(y_true, y_pred):
    """ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸”ì„ ê¸°ë°˜ìœ¼ë¡œ F1 ë° Accuracyë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    accuracy = np.mean(y_true == y_pred)
    f1 = f1_score(y_true, y_pred, average='binary')

    return {
        'accuracy': accuracy,
        'f1': f1,
    }


# ===================================================================
# 2. í‰ê°€ í•¨ìˆ˜ (PyTorch ìˆ˜ë™ ì¶”ë¡ )
# ===================================================================

def evaluate_judges():
    """
    CPU Only í™˜ê²½ì—ì„œ Trainer/accelerate ì—†ì´ PyTorch DataLoaderë¥¼ ì‚¬ìš©í•˜ì—¬
    ëª¨ë“  Judge ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
    """
    print("\n\n=============================================")
    print("âœ¨ ì‹¬ì‚¬ê´€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘ (PyTorch ìˆ˜ë™ ëª¨ë“œ) âœ¨")
    print("=============================================")

    all_results = []

    # ëª¨ë¸ í´ë˜ìŠ¤ ê²°ì • (MODEL_NAMEì— ë”°ë¼ ë™ì  ê²°ì •)
    model_name_lower = config.MODEL_NAME.lower()
    if "distil" in model_name_lower:
        MODEL_CLS = DistilBertForSequenceClassification
    elif "bert" in model_name_lower or "klue" in model_name_lower:
        MODEL_CLS = AutoModelForSequenceClassification
    else:
        MODEL_CLS = AutoModelForSequenceClassification
    print(f"[INFO] ëª¨ë¸ í´ë˜ìŠ¤: {MODEL_CLS.__name__}")

    # 1. í‰ê°€ìš© íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° í™•ì¸
    eval_json_path = os.path.join(config.DATA_EVAL_DIR, config.DATA_EVAL_FILE_NAME)
    if not os.path.exists(eval_json_path):
        print(f"[ERROR] í‰ê°€ì— ì‚¬ìš©ë  íŒŒì¼ {eval_json_path}ê°€ ì—†ì–´ì„œ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    raw_eval_data_all = load_jsonl_to_list(eval_json_path)

    # 2. PyTorch device ì„¤ì • (CPU Only)
    device = torch.device("cpu")
    print(f"[INFO] í‰ê°€ ì¥ì¹˜: {device}")

    for judge_type, text_a_key, text_b_key, label_key in JUDGE_TASKS:
        judge_lower = judge_type.lower()

        # 3. ëª¨ë¸ ê²½ë¡œ ìƒì„± ë° í™•ì¸ (ê·œì¹™: config.MODEL_DIR/judge_type)
        model_path = os.path.join(config.MODEL_DIR, judge_type)
        if not os.path.exists(model_path):
            print(f"[SKIP] ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœ±ë‹ˆë‹¤: {model_path}")
            continue

        print(f"\n--- {judge_type} Judge í‰ê°€ ---")
        print(f"[INFO] ëª¨ë¸ ê²½ë¡œ: {model_path}")

        try:
            # 4. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
            model = MODEL_CLS.from_pretrained(model_path, num_labels=2, trust_remote_code=True)
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            model.to(device)  # ëª¨ë¸ì„ CPUë¡œ ì´ë™
            model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (í•„ìˆ˜)

            # 5. í‰ê°€ ë°ì´í„° ì¤€ë¹„ ë° í•„í„°ë§
            eval_dataset_full = prepare_dataset(raw_eval_data_all, text_a_key, text_b_key, label_key)

            # í•„í„°ë§ ë¡œì§ (ìœ íš¨í•˜ì§€ ì•Šì€ ë¬¸ìì—´ ì œê±°)
            def is_valid_text(example):
                text_a_val = example.get('text_a')
                text_b_val = example.get('text_b')
                return isinstance(text_a_val, str) and isinstance(text_b_val,
                                                                  str) and text_a_val.strip() != "" and text_b_val.strip() != ""

            eval_dataset = eval_dataset_full.filter(is_valid_text)

            initial_eval_size = len(eval_dataset_full)
            valid_size = len(eval_dataset)
            print(f"[INFO] í‰ê°€ ë°ì´í„° í•„í„°ë§: {initial_eval_size}ê°œ -> {valid_size}ê°œ (ì œê±°ë¨: {initial_eval_size - valid_size}ê°œ)")

            # 6. í† í°í™” ë° DataLoader ì¤€ë¹„

            # í† í°í™” í•¨ìˆ˜ (PyTorch Tensor ë°˜í™˜)
            def tokenize_data(examples):
                tokenized = tokenizer(
                    examples['text_a'], examples.get('text_b', None),
                    truncation=True, max_length=MAX_SEQ_LENGTH, padding='max_length',
                    return_tensors='pt'
                )
                # í† í°í™” ê²°ê³¼ì˜ [1, MAX_SEQ_LENGTH] í˜•íƒœë¥¼ [MAX_SEQ_LENGTH]ë¡œ squeeze í•©ë‹ˆë‹¤.
                return {k: v.squeeze(0) for k, v in tokenized.items()}

            # ë°ì´í„°ì…‹ì„ í† í°í™” (mapì˜ batched=FalseëŠ” ë°ì´í„° ë¬´ê²°ì„±ì„ ìœ ì§€í•˜ëŠ” ë° ìœ ë¦¬)
            eval_tokenized = eval_dataset.map(tokenize_data, batched=False, remove_columns=['text_a', 'text_b'])

            # ğŸŒŸ [ìˆ˜ì • ì‹œì‘: NumPyë¥¼ í†µí•œ ì•ˆì „í•œ í…ì„œ ì¶”ì¶œ] ğŸŒŸ

            # 1. datasets.Datasetì„ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ Column ê°ì²´ë¥¼ í•´ì œ
            input_ids_np = np.array(eval_tokenized['input_ids'])
            attention_mask_np = np.array(eval_tokenized['attention_mask'])
            labels_np = np.array(eval_tokenized['labels'])

            # 2. NumPy ë°°ì—´ì„ PyTorch í…ì„œë¡œ ë³€í™˜
            input_ids = torch.tensor(input_ids_np).to(torch.long)
            attention_mask = torch.tensor(attention_mask_np).to(torch.long)
            labels = torch.tensor(labels_np).to(torch.long)

            # 3. token_type_ids ì²˜ë¦¬
            if 'token_type_ids' in eval_tokenized.column_names:
                token_type_ids_np = np.array(eval_tokenized['token_type_ids'])
                token_type_ids = torch.tensor(token_type_ids_np).to(torch.long)

                # PyTorch Dataset ê°ì²´ ìƒì„± (4ê°œì˜ í…ì„œ)
                eval_data = TensorDataset(input_ids, attention_mask, token_type_ids, labels)
            else:
                # PyTorch Dataset ê°ì²´ ìƒì„± (3ê°œì˜ í…ì„œ)
                eval_data = TensorDataset(input_ids, attention_mask, labels)

            # ğŸŒŸ [ìˆ˜ì • ë] ğŸŒŸ

            eval_dataloader = DataLoader(
                eval_data,
                sampler=SequentialSampler(eval_data),
                batch_size=EVAL_BATCH_SIZE
            )

            # 7. ì¶”ë¡  ì‹¤í–‰ (ìˆ˜ë™ ë°°ì¹˜ ì²˜ë¦¬)
            y_pred_list = []
            y_true_list = []

            print(f"[INFO] {judge_type} ì˜ˆì¸¡ ì‹œì‘ ({len(eval_dataloader)} ë°°ì¹˜)")

            for batch in eval_dataloader:
                # ë°ì´í„°ë¥¼ CPUë¡œ ì´ë™ (GPUê°€ ì—†ìœ¼ë¯€ë¡œ)
                batch = tuple(t.to(device) for t in batch)

                # token_type_idsê°€ ìˆì„ ê²½ìš° inputsì— ì¶”ê°€
                inputs = {
                    'input_ids': batch[0],
                    'attention_mask': batch[1],
                }

                # TensorDatasetì— token_type_idsê°€ í¬í•¨ë˜ë©´ batch[2]ì—, ì•„ë‹ˆë©´ labelsëŠ” batch[2]
                if len(batch) == 4:
                    inputs['token_type_ids'] = batch[2]
                    labels = batch[3].cpu().numpy()
                else:
                    labels = batch[2].cpu().numpy()

                with torch.no_grad():
                    outputs = model(**inputs)

                logits = outputs.logits.cpu().numpy()
                predictions = np.argmax(logits, axis=1)

                y_pred_list.extend(predictions)
                y_true_list.extend(labels)

            # 8. ì§€í‘œ ê³„ì‚° (ê°œìˆ˜ ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„± ì—†ìŒ)
            y_true_final = np.array(y_true_list)
            y_pred_final = np.array(y_pred_list)

            # ìµœì¢… ìœ íš¨ ë°ì´í„° ê°œìˆ˜ í™•ì¸
            if len(y_true_final) != valid_size:
                print(f"[WARN] ì˜ˆì¸¡/ë ˆì´ë¸” ê°œìˆ˜ ë¶ˆì¼ì¹˜ ë°œìƒ! (ì˜ˆì¸¡: {len(y_pred_final)}, ê¸°ëŒ€: {valid_size})")

            metrics = compute_metrics(y_true_final, y_pred_final)

            result = {
                'Judge': judge_type,
                'F1-Score': metrics.get('f1'),
                'Accuracy': metrics.get('accuracy'),
                'Path': model_path
            }
            all_results.append(result)

            print(f"[RESULT] {judge_type} F1 Score: {result['F1-Score']:.4f}, Accuracy = {result['Accuracy']:.4f}")

        except Exception as e:
            print(f"[ERROR] {judge_type} ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            all_results.append({'Judge': judge_type, 'Error': str(e), 'Path': model_path})

    # 9. ìµœì¢… ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n=============================================")
    print("âœ… ë””ë ‰í† ë¦¬ í‰ê°€ ìµœì¢… ìš”ì•½")
    print("=============================================")
    for res in all_results:
        if 'F1-Score' in res:
            print(f"[{res['Judge']}]: F1 Score = {res['F1-Score']:.4f}, Accuracy = {res['Accuracy']:.4f}")
        else:
            print(f"[{res['Judge']}]: í‰ê°€ ì‹¤íŒ¨ - {res['Error']}")
    print("=============================================")

    return all_results


if __name__ == "__main__":
    # âš ï¸ NOTE: ì´ ì„¹ì…˜ì€ config ëª¨ë“ˆì´ ì •ì˜ë˜ì–´ ìˆê³ ,
    # í•„ìš”í•œ ë°ì´í„°ì™€ ëª¨ë¸ì´ ê²½ë¡œì— ìˆë‹¤ê³  ê°€ì •í•˜ê³  ì‹¤í–‰ë©ë‹ˆë‹¤.

    final_metrics = evaluate_judges()
    if not final_metrics:
        print(f"{final_metrics}")

    print("ìµœì¢… í‰ê°€ í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤. config ëª¨ë“ˆê³¼ í•¨ê»˜ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
