import json
import torch
from transformers import AutoTokenizer, DistilBertForSequenceClassification, AutoModelForSequenceClassification, \
    TrainingArguments, Trainer
import numpy as np
from datasets import Dataset
from sklearn.metrics import f1_score
import os
import logging
# âš ï¸ NOTE: config ëª¨ë“ˆì´ ì™¸ë¶€ì— ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
import config
from torch.utils.data import DataLoader, SequentialSampler, TensorDataset

# --- ë¡œê¹… ì„¤ì •: í† í°í™” ê²½ê³  ë©”ì‹œì§€ (Warning) ìˆ¨ê¸°ê¸° ---
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)

# ===================================================================
# 0. ì „ì—­ ìƒìˆ˜ ì„¤ì • (config ëª¨ë“ˆì—ì„œ ê°€ì ¸ì˜¨ë‹¤ê³  ê°€ì •)
# ===================================================================
# âš ï¸ NOTE: config.MODEL_NAME ë“±ì€ ì™¸ë¶€ config íŒŒì¼ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
# ì˜ˆì‹œ:
# MODEL_NAME = "monologg/distilkobert"
# DATA_EVAL_DIR = "/data/eval"
# DATA_EVAL_FILE_NAME = "eval_data.jsonl"
# MODEL_DIR = "/models"

NUM_TRAIN_EPOCHS = 5
TRAIN_BATCH_SIZE = 32
EVAL_BATCH_SIZE = 64
MAX_SEQ_LENGTH = 512

JUDGE_TASKS = [
    ('context_relevance', 'q', 'c', 'L_CR'),
    ('answer_faithfulness', 'c', 'a', 'L_AF'),
    ('answer_relevance', 'q', 'a', 'L_AR'),
]


# ===================================================================
# 1. ë³´ì¡° í•¨ìˆ˜
# ===================================================================

def load_jsonl_to_list(file_path):
    """ì§€ì •ëœ JSONL íŒŒì¼ ê²½ë¡œë¡œë¶€í„° ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    all_data = []
    if not os.path.exists(file_path):
        file_name = os.path.basename(file_path)
        print(f"[ERROR] íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_name} at {file_path}")
        return []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                all_data.append(json.loads(line))
        file_name = os.path.basename(file_path)
        print(f"[INFO] {file_name} íŒŒì¼ì—ì„œ ì´ {len(all_data)}ê°œì˜ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ.")
        return all_data
    except Exception as e:
        print(f"[ERROR] JSONL íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


def prepare_dataset(data_list, text_a_key, text_b_key, label_key):
    """
    ë¡œë“œëœ ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ Hugging Face Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    (pandas ì‚¬ìš© ì—†ì´, Dataset.from_dict ì‚¬ìš©)
    """
    dataset_dict = {
        'text_a': [],
        'text_b': [],
        'labels': []
    }
    for item in data_list:
        # NOTE: .get()ì„ ì‚¬ìš©í•˜ì—¬ í‚¤ê°€ ì—†ì„ ë•Œ ì˜¤ë¥˜ ë°©ì§€
        dataset_dict['text_a'].append(item.get(text_a_key))
        dataset_dict['text_b'].append(item.get(text_b_key))
        dataset_dict['labels'].append(item.get(label_key))

    return Dataset.from_dict(dataset_dict)


def compute_metrics(y_true, y_pred):
    """ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸”ì„ ê¸°ë°˜ìœ¼ë¡œ F1 ë° Accuracyë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    accuracy = np.mean(y_true == y_pred)
    f1 = f1_score(y_true, y_pred, average='binary')

    return {
        'accuracy': accuracy,
        'f1': f1,
    }


# ===================================================================
# 2. í‰ê°€ í•¨ìˆ˜ (PyTorch ìˆ˜ë™ ì¶”ë¡ )
# ===================================================================

def evaluate_judges():
    """
    CPU Only í™˜ê²½ì—ì„œ Trainer/accelerate ì—†ì´ PyTorch DataLoaderë¥¼ ì‚¬ìš©í•˜ì—¬
    ëª¨ë“  Judge ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
    """
    print("\n\n=============================================")
    print("âœ¨ ì‹¬ì‚¬ê´€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘ (PyTorch ìˆ˜ë™ ëª¨ë“œ) âœ¨")
    print("=============================================")

    all_results = []

    # ëª¨ë¸ í´ë˜ìŠ¤ ê²°ì • (MODEL_NAMEì— ë”°ë¼ ë™ì  ê²°ì •)
    model_name_lower = config.MODEL_NAME.lower()

    is_distilbert = "distil" in model_name_lower  # ğŸŒŸ DistilBERT ì—¬ë¶€ í”Œë˜ê·¸

    if is_distilbert:
        MODEL_CLS = DistilBertForSequenceClassification
    elif "bert" in model_name_lower or "klue" in model_name_lower:
        MODEL_CLS = AutoModelForSequenceClassification
    else:
        MODEL_CLS = AutoModelForSequenceClassification
    print(f"[INFO] ëª¨ë¸ í´ë˜ìŠ¤: {MODEL_CLS.__name__}")
    print(f"[INFO] DistilBERT ëª¨ë“œ: {is_distilbert}")

    # 1. í‰ê°€ìš© íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° í™•ì¸
    eval_json_path = os.path.join(config.DATA_EVAL_DIR, config.DATA_EVAL_FILE_NAME)
    if not os.path.exists(eval_json_path):
        print(f"[ERROR] í‰ê°€ì— ì‚¬ìš©ë  íŒŒì¼ {eval_json_path}ê°€ ì—†ì–´ì„œ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    raw_eval_data_all = load_jsonl_to_list(eval_json_path)

    # 2. PyTorch device ì„¤ì • (CPU Only)
    device = torch.device("cpu")
    print(f"[INFO] í‰ê°€ ì¥ì¹˜: {device}")

    for judge_type, text_a_key, text_b_key, label_key in JUDGE_TASKS:
        judge_lower = judge_type.lower()

        # 3. ëª¨ë¸ ê²½ë¡œ ìƒì„± ë° í™•ì¸ (ê·œì¹™: config.MODEL_DIR/judge_type)
        model_path = os.path.join(config.MODEL_DIR, judge_type)
        if not os.path.exists(model_path):
            print(f"[SKIP] ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœ±ë‹ˆë‹¤: {model_path}")
            continue

        print(f"\n--- {judge_type} Judge í‰ê°€ ---")
        print(f"[INFO] ëª¨ë¸ ê²½ë¡œ: {model_path}")

        try:
            # 4. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
            model = MODEL_CLS.from_pretrained(model_path, num_labels=2, trust_remote_code=True)
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            model.to(device)
            model.eval()

            # 5. í‰ê°€ ë°ì´í„° ì¤€ë¹„ ë° í•„í„°ë§
            eval_dataset_full = prepare_dataset(raw_eval_data_all, text_a_key, text_b_key, label_key)

            # í•„í„°ë§ ë¡œì§
            def is_valid_text(example):
                text_a_val = example.get('text_a')
                text_b_val = example.get('text_b')
                return isinstance(text_a_val, str) and isinstance(text_b_val,
                                                                  str) and text_a_val.strip() != "" and text_b_val.strip() != ""

            eval_dataset = eval_dataset_full.filter(is_valid_text)

            initial_eval_size = len(eval_dataset_full)
            valid_size = len(eval_dataset)
            print(f"[INFO] í‰ê°€ ë°ì´í„° í•„í„°ë§: {initial_eval_size}ê°œ -> {valid_size}ê°œ (ì œê±°ë¨: {initial_eval_size - valid_size}ê°œ)")

            # 6. í† í°í™” ë° DataLoader ì¤€ë¹„

            # í† í°í™” í•¨ìˆ˜ (PyTorch Tensor ë°˜í™˜)
            def tokenize_data(examples):
                tokenized = tokenizer(
                    examples['text_a'], examples.get('text_b', None),
                    truncation=True, max_length=MAX_SEQ_LENGTH, padding='max_length',
                    return_tensors='pt'
                )
                return {k: v.squeeze(0) for k, v in tokenized.items()}

            eval_tokenized = eval_dataset.map(tokenize_data, batched=False, remove_columns=['text_a', 'text_b'])

            # 7. NumPyë¥¼ í†µí•œ ì•ˆì „í•œ í…ì„œ ì¶”ì¶œ

            # 1. datasets.Datasetì„ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ Column ê°ì²´ë¥¼ í•´ì œ
            # í…ì„œì— í¬í•¨ë  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
            tensor_np_cols = ['input_ids', 'attention_mask']
            if not is_distilbert and 'token_type_ids' in eval_tokenized.column_names:
                tensor_np_cols.append('token_type_ids')

            eval_tokenized.set_format(type='numpy', columns=tensor_np_cols + ['labels'])

            # ğŸŒŸ [ìˆ˜ì • ì‹œì‘]: torch.tensor() ëŒ€ì‹  np.array()ë¥¼ ê±°ì³ í•˜ë‚˜ì˜ í…ì„œë¡œ ë³€í™˜ ğŸŒŸ
            input_ids = torch.tensor(np.array(eval_tokenized['input_ids'])).to(torch.long)
            attention_mask = torch.tensor(np.array(eval_tokenized['attention_mask'])).to(torch.long)
            labels = torch.tensor(np.array(eval_tokenized['labels'])).to(torch.long)
            # ğŸŒŸ [ìˆ˜ì • ë] ğŸŒŸ

            # 8. TensorDataset ê°ì²´ ìƒì„± (DistilBERT ì¡°ê±´ ë°˜ì˜)
            if not is_distilbert and 'token_type_ids' in eval_tokenized.column_names:
                # DistilBERTê°€ ì•„ë‹ ë•Œë§Œ token_type_ids ì‚¬ìš©
                token_type_ids = torch.tensor(np.array(eval_tokenized['token_type_ids'])).to(torch.long)
                eval_data = TensorDataset(input_ids, attention_mask, token_type_ids, labels)
                is_token_type_used = True
            else:
                # DistilBERTì´ê±°ë‚˜ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ), token_type_ids ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                eval_data = TensorDataset(input_ids, attention_mask, labels)
                is_token_type_used = False

            print(f"[INFO] token_type_ids ì‚¬ìš© ì—¬ë¶€: {is_token_type_used}")

            eval_dataloader = DataLoader(
                eval_data,
                sampler=SequentialSampler(eval_data),
                batch_size=EVAL_BATCH_SIZE
            )

            # 9. ì¶”ë¡  ì‹¤í–‰ (ìˆ˜ë™ ë°°ì¹˜ ì²˜ë¦¬)
            y_pred_list = []
            y_true_list = []

            print(f"[INFO] {judge_type} ì˜ˆì¸¡ ì‹œì‘ ({len(eval_dataloader)} ë°°ì¹˜)")

            for batch in eval_dataloader:
                print(".", end='')  # ì•„ë¬´ê²ƒë„ ì•ˆ ë³´ì´ë©´ ë‹µë‹µí•˜ë‹ˆê¹Œ
                # ë°ì´í„°ë¥¼ CPUë¡œ ì´ë™ (GPUê°€ ì—†ìœ¼ë¯€ë¡œ)
                batch = tuple(t.to(device) for t in batch)

                inputs = {
                    'input_ids': batch[0],
                    'attention_mask': batch[1],
                }

                # TensorDatasetì— token_type_idsê°€ í¬í•¨ëœ ê²½ìš°
                if is_token_type_used:
                    inputs['token_type_ids'] = batch[2]
                    labels = batch[3].cpu().numpy()
                else:
                    # token_type_idsê°€ ì—†ëŠ” ê²½ìš° (DistilBERT í¬í•¨)
                    # labelsëŠ” batch[2]ì— ìœ„ì¹˜
                    labels = batch[2].cpu().numpy()

                with torch.no_grad():
                    outputs = model(**inputs)

                logits = outputs.logits.cpu().numpy()
                predictions = np.argmax(logits, axis=1)

                y_pred_list.extend(predictions)
                y_true_list.extend(labels)

            # 10. ì§€í‘œ ê³„ì‚°
            y_true_final = np.array(y_true_list)
            y_pred_final = np.array(y_pred_list)

            # ìµœì¢… ìœ íš¨ ë°ì´í„° ê°œìˆ˜ í™•ì¸
            if len(y_true_final) != valid_size:
                print(f"\n[WARN] ì˜ˆì¸¡/ë ˆì´ë¸” ê°œìˆ˜ ë¶ˆì¼ì¹˜ ë°œìƒ! (ì˜ˆì¸¡: {len(y_pred_final)}, ê¸°ëŒ€: {valid_size})")

            metrics = compute_metrics(y_true_final, y_pred_final)

            result = {
                'Judge': judge_type,
                'F1-Score': metrics.get('f1'),
                'Accuracy': metrics.get('accuracy'),
                'Path': model_path
            }
            all_results.append(result)

            print(f"\n[RESULT] {judge_type} F1 Score: {result['F1-Score']:.4f}, Accuracy = {result['Accuracy']:.4f}")

        except Exception as e:
            print(f"\n[ERROR] {judge_type} ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            all_results.append({'Judge': judge_type, 'Error': str(e), 'Path': model_path})

    # 11. ìµœì¢… ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n=============================================")
    print("âœ… ë””ë ‰í† ë¦¬ í‰ê°€ ìµœì¢… ìš”ì•½")
    print("=============================================")
    for res in all_results:
        if 'F1-Score' in res:
            print(f"[{res['Judge']}]: F1 Score = {res['F1-Score']:.4f}, Accuracy = {res['Accuracy']:.4f}")
        else:
            print(f"[{res['Judge']}]: í‰ê°€ ì‹¤íŒ¨ - {res['Error']}")
    print("=============================================")

    return all_results


if __name__ == "__main__":
    final_metrics = evaluate_judges()
    if not final_metrics:
        print(f"{final_metrics}")
